<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="KrorngAI">
<meta name="dcterms.date" content="2026-02-01">
<meta name="description" content="Re-implementing PARSeq Model from scratch for Khmer text">

<title>Scence Text Recognition with Permuted Autoregressive Sequence Models – KrorngAI</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-39d349927537621b566d286ac43747d6.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="quarto-light">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#abstract" id="toc-abstract" class="nav-link active" data-scroll-target="#abstract">Abstract</a></li>
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#core-idea" id="toc-core-idea" class="nav-link" data-scroll-target="#core-idea">Core Idea</a>
  <ul class="collapse">
  <li><a href="#objective-function" id="toc-objective-function" class="nav-link" data-scroll-target="#objective-function">Objective Function</a></li>
  <li><a href="#tricks-in-implementation" id="toc-tricks-in-implementation" class="nav-link" data-scroll-target="#tricks-in-implementation">Tricks in Implementation</a>
  <ul class="collapse">
  <li><a href="#manipulate-attention-mask" id="toc-manipulate-attention-mask" class="nav-link" data-scroll-target="#manipulate-attention-mask">Manipulate Attention Mask</a></li>
  <li><a href="#use-empirical-version-of-eq-plm" id="toc-use-empirical-version-of-eq-plm" class="nav-link" data-scroll-target="#use-empirical-version-of-eq-plm">Use Empirical version of Equation&nbsp;2</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#implementation" id="toc-implementation" class="nav-link" data-scroll-target="#implementation">Implementation</a>
  <ul class="collapse">
  <li><a href="#mixed-precision-training" id="toc-mixed-precision-training" class="nav-link" data-scroll-target="#mixed-precision-training">Mixed-Precision Training</a></li>
  <li><a href="#image-encoder" id="toc-image-encoder" class="nav-link" data-scroll-target="#image-encoder">Image Encoder</a></li>
  <li><a href="#residual-attention-block" id="toc-residual-attention-block" class="nav-link" data-scroll-target="#residual-attention-block">Residual Attention Block</a></li>
  <li><a href="#visio-lingual-decoder" id="toc-visio-lingual-decoder" class="nav-link" data-scroll-target="#visio-lingual-decoder">Visio-lingual Decoder</a></li>
  <li><a href="#parseq-model" id="toc-parseq-model" class="nav-link" data-scroll-target="#parseq-model">PARSeq Model</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Scence Text Recognition with Permuted Autoregressive Sequence Models</h1>
  <div class="quarto-categories">
    <div class="quarto-category">Science</div>
    <div class="quarto-category">Technology</div>
  </div>
  </div>

<div>
  <div class="description">
    Re-implementing PARSeq Model from scratch for Khmer text
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>KrorngAI </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">February 1, 2026</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="abstract" class="level1">
<h1>Abstract</h1>
<p>I want to re-implement PARSeq (<span class="citation" data-cites="bautista2022scene">Bautista and Atienza (<a href="#ref-bautista2022scene" role="doc-biblioref">2022</a>)</span>) (<a href="https://github.com/baudm/parseq">https://github.com/baudm/parseq)</a> for Khmer text. Using <a href="https://huggingface.co/datasets/Kimang18/khmer-hanuman-small-images">Kimang18/khmer-hanuman-small-images</a>, I got <code>cer</code> of <span class="math inline">\(70\%\)</span> which is not sufficiently good compared to the performance of PARSeq presented in their paper.</p>
</section>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>PARSeq is a method to solve the task of recognizing text from the cropped regions in natural scenes. This method achieves state-of-the-art (SOTA) results in STR benchmarks (<span class="math inline">\(91.9\%\)</span> accuracy) by just using synthetic training data <span class="citation" data-cites="bautista2022scene">Bautista and Atienza (<a href="#ref-bautista2022scene" role="doc-biblioref">2022</a>)</span>.</p>
</section>
<section id="core-idea" class="level1">
<h1>Core Idea</h1>
<p>In sequence modeling, deep learning models are trained to generate <em>future</em> tokens conditioned on <em>past</em> tokens. Consequently, the models become <span id="eq-ar"><span class="math display">\[
\mathbb{P}(\mathbf{y}|\mathbf{x})
=\prod_{t=1}^T\mathbb{P}(y_t|\mathbf{y}_{&lt;t}, \mathbf{x})
=p(y_1|\mathbf{x})\times p(y_2|y_1, \mathbf{x})\times p(y_3|y_2, y_1, \mathbf{x})\times \dots \times p(y_T|y_{T-1},\dots, y_1, \mathbf{x})
\tag{1}\]</span></span> where <span class="math inline">\(\mathbf{y}\)</span> is the <span class="math inline">\(T\)</span>-length text label of the image <span class="math inline">\(\mathbf{x}\)</span>. This method is known as <em>monotonic</em> autoregressive generation. We want to find parameter <span class="math inline">\(\theta^*\)</span> that maximizes <a href="#eq-ar" class="quarto-xref">Equation&nbsp;1</a> <span class="math display">\[\begin{equation}
\log \mathbb{P}_{\theta^*}(\mathbf{y} | \mathbf{x}) = \max_{\theta}\sum_{t=1}^T\log p_{\theta}(y_t|\mathbf{y}_{&lt;t}, \mathbf{x})
\end{equation}\]</span></p>
<p>PARSeq method is an adaptation of Permutation Language Modeling (PLM) (<span class="citation" data-cites="yang2019generalized">Yang et al. (<a href="#ref-yang2019generalized" role="doc-biblioref">2019</a>)</span>) for Scene Text Recognition (STR). Let <span class="math inline">\({\mathcal{Z}}_T\)</span> be the set of all possible permutations of <span class="math inline">\(\{1,2,\dots,T\}\)</span>. Without loss of generality, we will fix <span class="math inline">\(T\)</span> and omit it from the expression if possible. Let <span class="math inline">\({\mathbf{z}}\in{\mathcal{Z}}\)</span> be a permutation that specify an ordering of the text label <span class="math inline">\({\mathbf{y}}\)</span> and define</p>
<p><span class="math display">\[\begin{align*}
\mathbb{P}(\mathbf{y}_{{\mathbf{z}}}|\mathbf{x})
&amp;=p(y_{z_1}|\mathbf{x})\times p(y_{z_2}|y_{z_1}, \mathbf{x})\times p(y_{z_3}|y_{z_2}, y_{z_1}, \mathbf{x})\times \dots \times p(y_{z_T}|y_{z_{T-1}},\dots, y_{z_1}, \mathbf{x}) \\
&amp;=\prod_{t=1}^Tp(y_{z_t}|{\mathbf{y}}_{{\mathbf{z}}_{&lt;t}}{\mathbf{x}})
\end{align*}\]</span> So, <a href="#eq-ar" class="quarto-xref">Equation&nbsp;1</a> is the case where <span class="math inline">\(z_t=t, \forall t\le T\)</span>.</p>
<p>PARSeq method wants to find <span class="math inline">\(\theta^*\)</span> that maximizes</p>
<p><span id="eq-plm"><span class="math display">\[
\log \mathbb{P}_{\theta^*}({\mathbf{y}}| {\mathbf{x}}) = \max_{\theta}{\mathbb{E}}_{{\mathbf{z}}\in{\mathcal{Z}}}\left[\sum_{t=1}^T\log p_{\theta}(y_{z_t}|\mathbf{y}_{z_{&lt;t}}, \mathbf{x})\right]
\tag{2}\]</span></span></p>
<p>It is clear that PARSeq is more general than <em>monotonic</em> autoregressive method that execute the prediction in only one direction, namely left-to-right. Moreover, the objective function <a href="#eq-plm" class="quarto-xref">Equation&nbsp;2</a> only requires modification on <em>Visio-lingual Decoder</em> (<em>Image Encoder</em> can follow similarly to Vision Transformer architecture). An overview of PARSeq architecture and training process is given in <a href="#fig-archit" class="quarto-xref">Figure&nbsp;1</a>.</p>
<section id="objective-function" class="level2">
<h2 class="anchored" data-anchor-id="objective-function">Objective Function</h2>
<p>Let <span class="math inline">\({\mathbf{e}}\in{\mathbb{R}}^{T\times d_m}\)</span> be the token embedding of <span class="math inline">\({\mathbf{y}}\in{\mathbb{R}}^T\)</span>, <span class="math inline">\({\mathbf{p}}\in{\mathbb{R}}^{T\times d_m}\)</span> be the position embedding and <span class="math inline">\({\mathbf{c}}:={\mathbf{e}}+{\mathbf{p}}\)</span> be the context embedding. <span class="math inline">\(d_m\)</span> is the embedding dimension. Let <span class="math inline">\({\mathbf{M}}_k\)</span> be the attention mask that reflect the permutation <span class="math inline">\({\mathbf{z}}_k\)</span> (<em>e.g.</em> if <span class="math inline">\(\forall t\le T, z_{k,t}=t\)</span>, then the corresponding mask is just the causal mask for left-to-right direction). The model <span class="math inline">\(f({\mathbf{x}}, {\mathbf{p}}, {\mathbf{c}}, {\mathbf{M}})\)</span> outputs logits in space <span class="math inline">\({\mathbb{R}}^{(T+1)\times(S+1)}\)</span> where <span class="math inline">\(S\)</span> is the size of character set (charset) used for training with additional character pertaining to <span class="math inline">\([E]\)</span> token (which marks the end of sequence). Applying softmax on the logits, we get the probability distribution over charset.</p>
<p>The loss for a given <span class="math inline">\({\mathbf{M}}_k\)</span> is just a cross-entropy loss <span class="math inline">\({\mathcal{L}}_{ce}\left({\mathbf{y}}, f({\mathbf{x}}, {\mathbf{p}}, {\mathbf{c}}, {\mathbf{M}}_k)\right)\)</span> and the final loss is given by <span id="eq-obj"><span class="math display">\[
{\mathcal{L}}=\frac{1}{K}\sum_{k=1}^K{\mathcal{L}}_{ce}\left({\mathbf{y}}, f({\mathbf{x}}, {\mathbf{p}}, {\mathbf{c}}, {\mathbf{M}}_k)\right)
\tag{3}\]</span></span></p>
<p>So, the main implementation is to generate permutation <span class="math inline">\({\mathbf{z}}_k\)</span> and the corresponding attention mask <span class="math inline">\({\mathbf{M}}_k\)</span> for <span class="math inline">\(1\le k\le K\)</span>.</p>
<div id="fig-archit" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-archit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./figures/model_architecture.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-archit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: PARSeq architecture and training overview. <em>LayerNorm</em> and <em>Dropout</em> layers are ommitted due to space constraints, <span class="math inline">\([B]\)</span>, <span class="math inline">\([E]\)</span>, and <span class="math inline">\([P]\)</span> stand for <em>beginning-of-sequence (BOS)</em>, <em>end-of-sequence (EOS)</em>, and <em>padding</em> tokens, respectively. <span class="math inline">\(T=25\)</span> results in 26 distinct <em>position</em> tokens. The position tokens both serve as query vectors and position embeddings for the input context. For <span class="math inline">\([B]\)</span>, no position embedding is added. Attention masks are generated from the given permutations and are used only for the <em>context-position</em> attention. <span class="math inline">\({\mathcal{L}}_{ce}\)</span> pertains to the cross-entropy loss.
</figcaption>
</figure>
</div>
</section>
<section id="tricks-in-implementation" class="level2">
<h2 class="anchored" data-anchor-id="tricks-in-implementation">Tricks in Implementation</h2>
<p>There are two tricks to make the PARSeq’s implementation more feasible:</p>
<ol type="1">
<li>Manipulate Attention Mask</li>
<li>Use empirical version of <a href="#eq-plm" class="quarto-xref">Equation&nbsp;2</a></li>
</ol>
<section id="manipulate-attention-mask" class="level3">
<h3 class="anchored" data-anchor-id="manipulate-attention-mask">Manipulate Attention Mask</h3>
<p>In PARSeq, the <em>position</em> tokens encode the target position to be predicted, each one having a direct correspondence to a specific position in the output. The model uses position tokens as the query for the attention mechanism, word combined with position tokens as context embedding (namely, key and value), and mask attention to reflect the ordering specified by permutation. Using position tokens as the query allows the model to learn meaningful pattern from PLM(<span class="citation" data-cites="bautista2022scene">Bautista and Atienza (<a href="#ref-bautista2022scene" role="doc-biblioref">2022</a>)</span>). This is different from standard AR model where context embedding as the query (<em>aka</em> self-attention mechanism).</p>
<div id="tbl-panel" class="quarto-layout-panel anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-panel-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: Illustration of AR attention masks for each permutation. The table header (with the <span class="math inline">\([B]\)</span> token) pertains to the input context, while the header column (with the <span class="math inline">\([E]\)</span> token at position <span class="math inline">\(p_4\)</span>) corresponds to the output tokens. <span class="math inline">\(0\)</span> means that the output token has conditional dependency on the corresponding input token. <span class="math inline">\(-\inf\)</span> means that no information flows from input to output. Note that regardless of permutations, the position <span class="math inline">\(p_4\)</span> corresponding to <span class="math inline">\([E]\)</span> token has information from all input tokens and information of <span class="math inline">\([B]\)</span> token flows to all position tokens.
</figcaption>
<div aria-describedby="tbl-panel-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="tbl-panel" style="flex-basis: 25.0%;justify-content: center;">
<div id="tbl-first" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-tbl figure">
<figcaption class="quarto-float-caption-top quarto-subfloat-caption quarto-subfloat-tbl" id="tbl-first-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) <span class="math inline">\([1,2,3]\)</span>
</figcaption>
<div aria-describedby="tbl-first-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;"><span class="math inline">\([B]\)</span></th>
<th style="text-align: right;"><span class="math inline">\(c_1\)</span></th>
<th style="text-align: right;"><span class="math inline">\(c_2\)</span></th>
<th style="text-align: right;"><span class="math inline">\(c_3\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(p_1\)</span></td>
<td style="text-align: right;">0</td>
<td style="text-align: right;"><span class="math inline">\(-\inf\)</span></td>
<td style="text-align: right;"><span class="math inline">\(-\inf\)</span></td>
<td style="text-align: right;"><span class="math inline">\(-\inf\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(p_2\)</span></td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;"><span class="math inline">\(-\inf\)</span></td>
<td style="text-align: right;"><span class="math inline">\(-\inf\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(p_3\)</span></td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;"><span class="math inline">\(-\inf\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(p_4\)</span></td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="tbl-panel" style="flex-basis: 25.0%;justify-content: center;">
<div id="tbl-second" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-tbl figure">
<figcaption class="quarto-float-caption-top quarto-subfloat-caption quarto-subfloat-tbl" id="tbl-second-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) <span class="math inline">\([3,2,1]\)</span>
</figcaption>
<div aria-describedby="tbl-second-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;"><span class="math inline">\([B]\)</span></th>
<th style="text-align: right;"><span class="math inline">\(c_1\)</span></th>
<th style="text-align: right;"><span class="math inline">\(c_2\)</span></th>
<th style="text-align: right;"><span class="math inline">\(c_3\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(p_1\)</span></td>
<td style="text-align: right;">0</td>
<td style="text-align: right;"><span class="math inline">\(-\inf\)</span></td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(p_2\)</span></td>
<td style="text-align: right;">0</td>
<td style="text-align: right;"><span class="math inline">\(-\inf\)</span></td>
<td style="text-align: right;"><span class="math inline">\(-\inf\)</span></td>
<td style="text-align: right;">0</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(p_3\)</span></td>
<td style="text-align: right;">0</td>
<td style="text-align: right;"><span class="math inline">\(-\inf\)</span></td>
<td style="text-align: right;"><span class="math inline">\(-\inf\)</span></td>
<td style="text-align: right;"><span class="math inline">\(-\inf\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(p_4\)</span></td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="tbl-panel" style="flex-basis: 25.0%;justify-content: center;">
<div id="tbl-third" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-tbl figure">
<figcaption class="quarto-float-caption-top quarto-subfloat-caption quarto-subfloat-tbl" id="tbl-third-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(c) <span class="math inline">\([1,3,2]\)</span>
</figcaption>
<div aria-describedby="tbl-third-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;"><span class="math inline">\([B]\)</span></th>
<th style="text-align: right;"><span class="math inline">\(c_1\)</span></th>
<th style="text-align: right;"><span class="math inline">\(c_2\)</span></th>
<th style="text-align: right;"><span class="math inline">\(c_3\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(p_1\)</span></td>
<td style="text-align: right;">0</td>
<td style="text-align: right;"><span class="math inline">\(-\inf\)</span></td>
<td style="text-align: right;"><span class="math inline">\(-\inf\)</span></td>
<td style="text-align: right;"><span class="math inline">\(-\inf\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(p_2\)</span></td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;"><span class="math inline">\(-\inf\)</span></td>
<td style="text-align: right;">0</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(p_3\)</span></td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;"><span class="math inline">\(-\inf\)</span></td>
<td style="text-align: right;"><span class="math inline">\(-\inf\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(p_4\)</span></td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="tbl-panel" style="flex-basis: 25.0%;justify-content: center;">
<div id="tbl-fourth" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-tbl figure">
<figcaption class="quarto-float-caption-top quarto-subfloat-caption quarto-subfloat-tbl" id="tbl-fourth-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(d) <span class="math inline">\([2,3,1]\)</span>
</figcaption>
<div aria-describedby="tbl-fourth-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;"><span class="math inline">\([B]\)</span></th>
<th style="text-align: right;"><span class="math inline">\(c_1\)</span></th>
<th style="text-align: right;"><span class="math inline">\(c_2\)</span></th>
<th style="text-align: right;"><span class="math inline">\(c_3\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(p_1\)</span></td>
<td style="text-align: right;">0</td>
<td style="text-align: right;"><span class="math inline">\(-\inf\)</span></td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(p_2\)</span></td>
<td style="text-align: right;">0</td>
<td style="text-align: right;"><span class="math inline">\(-\inf\)</span></td>
<td style="text-align: right;"><span class="math inline">\(-\inf\)</span></td>
<td style="text-align: right;"><span class="math inline">\(-\inf\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(p_3\)</span></td>
<td style="text-align: right;">0</td>
<td style="text-align: right;"><span class="math inline">\(-\inf\)</span></td>
<td style="text-align: right;">0</td>
<td style="text-align: right;"><span class="math inline">\(-\inf\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(p_4\)</span></td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</div>
</div>
</div>
</figure>
</div>
</section>
<section id="use-empirical-version-of-eq-plm" class="level3">
<h3 class="anchored" data-anchor-id="use-empirical-version-of-eq-plm">Use Empirical version of <a href="#eq-plm" class="quarto-xref">Equation&nbsp;2</a></h3>
<p>For <span class="math inline">\(T\)</span>-length text label, there are <span class="math inline">\(T!\)</span> factorizations of likelihood as <a href="#eq-plm" class="quarto-xref">Equation&nbsp;2</a>. This is not practical as <span class="math inline">\(T\)</span> tends to be large in practice. Moreover, doing <span class="math inline">\(T!\)</span> factorizations does not always guarantee better performance (<span class="citation" data-cites="bautista2022scene">Bautista and Atienza (<a href="#ref-bautista2022scene" role="doc-biblioref">2022</a>)</span>) and could cause training instability. So, the author makes a compromise by choosing only <span class="math inline">\(K=6\)</span> permutations including left-to-right and right-to-left directions.</p>
</section>
</section>
</section>
<section id="implementation" class="level1">
<h1>Implementation</h1>
<section id="mixed-precision-training" class="level2">
<h2 class="anchored" data-anchor-id="mixed-precision-training">Mixed-Precision Training</h2>
<p>I will use ready-implemented layers in PyTorch. However, those layers are implemented for <code>float32</code>. Since I use <code>bfloat16</code>, I need to re-implement those layers by casting <code>dtype</code> accordingly. If you do not use mixed-precision training, you can ignore the code in this section.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Optional, Sequence</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch, torch.nn <span class="im">as</span> nn</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> Tensor</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dataclasses <span class="im">import</span> dataclass</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> RMSNorm(nn.RMSNorm):</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">super</span>().forward(x.<span class="bu">float</span>()).<span class="bu">type</span>(x.dtype)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Linear(nn.Linear):</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: Tensor) <span class="op">-&gt;</span> Tensor:</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> F.linear(x, <span class="va">self</span>.weight.to(x.dtype), <span class="va">None</span> <span class="cf">if</span> <span class="va">self</span>.bias <span class="kw">is</span> <span class="va">None</span> <span class="cf">else</span> <span class="va">self</span>.bias.to(x.dtype))</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiheadAttention(nn.MultiheadAttention):</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, query, key, value, key_padding_mask<span class="op">=</span><span class="va">None</span>, need_weights<span class="op">=</span><span class="va">False</span>, attn_mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>        z1, z2 <span class="op">=</span> <span class="bu">super</span>().forward(query.<span class="bu">float</span>(), key.<span class="bu">float</span>(), value.<span class="bu">float</span>(), need_weights<span class="op">=</span>need_weights, key_padding_mask<span class="op">=</span>key_padding_mask, attn_mask<span class="op">=</span>attn_mask)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> z1.<span class="bu">type</span>(query.dtype), z2</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="at">@dataclass</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ModelConfig:</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    img_size: Sequence[<span class="bu">int</span>]</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    patch_size: Sequence[<span class="bu">int</span>]</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    n_channel: <span class="bu">int</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    vocab_size: <span class="bu">int</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    block_size: <span class="bu">int</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>    n_layer: <span class="bu">int</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>    n_head: <span class="bu">int</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>    n_embed: <span class="bu">int</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>    dropout: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>    bias: <span class="bu">bool</span> <span class="op">=</span> <span class="va">True</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="image-encoder" class="level2">
<h2 class="anchored" data-anchor-id="image-encoder">Image Encoder</h2>
<p>To facilitate code readibility, I use Vision Transformer implemented in PyTorch for Image Encoder of PARSeq. I follow PARSeq paper by taking image size as <span class="math inline">\((128, 32)\)</span> and patch size as <span class="math inline">\((8, 4)\)</span>. So, patch embedding outputs <span class="math inline">\(\frac{128}{8}\times \frac{32}{4}=128\)</span> tokens, each token is a vector of size <span class="math inline">\(d_{m}\)</span>.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> timm.models.vision_transformer <span class="im">import</span> PatchEmbed, VisionTransformer</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ImageEncoder(VisionTransformer):</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>            img_size<span class="op">=</span>config.img_size,</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>            patch_size<span class="op">=</span>config.patch_size,</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>            in_chans<span class="op">=</span>config.n_channel,</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>            embed_dim<span class="op">=</span>config.n_embed,</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>            depth<span class="op">=</span>config.n_layer,</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>            num_heads<span class="op">=</span>config.n_head,</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>            mlp_ratio<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>            qkv_bias<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>            drop_rate<span class="op">=</span><span class="fl">0.0</span>,</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>            attn_drop_rate<span class="op">=</span><span class="fl">0.0</span>,</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>            drop_path_rate<span class="op">=</span><span class="fl">0.0</span>,</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>            embed_layer<span class="op">=</span>PatchEmbed,</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>            num_classes<span class="op">=</span><span class="dv">0</span>, <span class="co"># These</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>            global_pool<span class="op">=</span><span class="st">''</span>, <span class="co"># disable the</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>            class_token<span class="op">=</span><span class="va">False</span>, <span class="co"># classifier head.</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.forward_features(x)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="residual-attention-block" class="level2">
<h2 class="anchored" data-anchor-id="residual-attention-block">Residual Attention Block</h2>
<p>I design this block for self-attention and causal cross-attention.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ResidualAttentionBlock(nn.Module):</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_embed, n_head, n_mlp, dropout<span class="op">=</span><span class="fl">0.0</span>, bias<span class="op">=</span><span class="va">True</span>, activation<span class="op">=</span><span class="va">None</span>, cross_attn<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_embed <span class="op">=</span> n_embed</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_head <span class="op">=</span> n_head</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout1 <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mha <span class="op">=</span> MultiheadAttention(n_embed, n_head, dropout<span class="op">=</span>dropout, bias<span class="op">=</span>bias, batch_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout2 <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cross_ln <span class="op">=</span> RMSNorm(n_embed) <span class="cf">if</span> cross_attn <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cross_mha <span class="op">=</span> MultiheadAttention(n_embed, n_head, dropout<span class="op">=</span>dropout, bias<span class="op">=</span>bias, batch_first<span class="op">=</span><span class="va">True</span>) <span class="cf">if</span> cross_attn <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ln <span class="op">=</span> RMSNorm(n_embed)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ffn <span class="op">=</span> nn.Sequential(</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>            Linear(n_embed, n_mlp, bias<span class="op">=</span>bias),</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>            nn.GELU(approximate<span class="op">=</span><span class="st">'tanh'</span>),</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>            nn.Dropout(dropout),</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>            Linear(n_mlp, n_embed, bias<span class="op">=</span>bias)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, ctx, xi, key_mask, attn_mask):</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a><span class="co">        x: normalized query tensor (b, t, n_embed)</span></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a><span class="co">        ctx: normalized context tensor (b, t, n_embed)</span></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a><span class="co">        xi: normalized image feature tensor (b, t, n_embed)</span></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>        b, t, _ <span class="op">=</span> x.size()</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>        residual <span class="op">=</span> x</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>        x, _ <span class="op">=</span> <span class="va">self</span>.mha(query<span class="op">=</span>x, key<span class="op">=</span>ctx, value<span class="op">=</span>ctx, key_padding_mask<span class="op">=</span>key_mask, need_weights<span class="op">=</span><span class="va">False</span>, attn_mask<span class="op">=</span>attn_mask)</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> residual <span class="op">+</span> <span class="va">self</span>.dropout1(x)</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.cross_mha:</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>            residual <span class="op">=</span> x</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> <span class="va">self</span>.cross_ln(x)</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>            x, _ <span class="op">=</span> <span class="va">self</span>.cross_mha(query<span class="op">=</span>x, key<span class="op">=</span>xi, value<span class="op">=</span>xi, need_weights<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> residual <span class="op">+</span> <span class="va">self</span>.dropout2(x)</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>        residual <span class="op">=</span> x</span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.ln(x)</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.ffn(x)</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> residual <span class="op">+</span> x</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="visio-lingual-decoder" class="level2">
<h2 class="anchored" data-anchor-id="visio-lingual-decoder">Visio-lingual Decoder</h2>
<p>For visio-lingual decoder, the heavy task is to implement attention mask according to the position permutation. As shown in <a href="#tbl-panel" class="quarto-xref">Table&nbsp;1</a>, the first column of the mask is for <code>bos</code>. Since any positions must communicate with <code>bos</code> token, <strong>the first column</strong> of any attention masks here is all <strong>zero</strong>. Moreover, the position corresponding to <code>eos</code> target token must communicate with all context tokens. So, the last row of any attention masks here is all <strong>zero</strong>. However, it is possible that the last position tokens correspond to <code>pad</code> target token. In such case, <code>handle_eos_masking</code> function makes sure that the positions corresponding to <code>eos</code> target token communicate with all context tokens.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_k_permutations(n, k):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Generates K unique permutations of the sequence [0, 1, ..., n-1].</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co">        n (int): The upper bound of the range (0 to n-1).</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co">        k (int): The number of permutations to generate.</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co">        list: A list containing K lists, each being a unique permutation.</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create the base sequence</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    base_list <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(n))</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use a set to ensure we get unique permutations if K is large</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># relative to the total possible permutations (n!)</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>    permutations <span class="op">=</span> <span class="bu">set</span>([<span class="bu">tuple</span>(base_list), <span class="bu">tuple</span>(<span class="bu">reversed</span>(base_list))])</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate n factorial to prevent infinite loops if K &gt; n!</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    max_possible <span class="op">=</span> math.factorial(n)</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> k <span class="op">&gt;</span> max_possible:</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="ss">f"Requested </span><span class="sc">{</span>k<span class="sc">}</span><span class="ss"> permutations, but only </span><span class="sc">{</span>max_possible<span class="sc">}</span><span class="ss"> exist for n=</span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">."</span>)</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="bu">len</span>(permutations) <span class="op">&lt;</span> k:</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># random.sample creates a new shuffled list without mutating the original</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>        perm <span class="op">=</span> random.sample(base_list, <span class="bu">len</span>(base_list))</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>        permutations.add(<span class="bu">tuple</span>(perm))</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>        permutations.add(<span class="bu">tuple</span>(<span class="bu">reversed</span>(perm)))</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [<span class="bu">list</span>(p) <span class="cf">for</span> p <span class="kw">in</span> permutations]</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_mask(permutation: Tensor):</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>    sz <span class="op">=</span> permutation.size(<span class="dv">0</span>)</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> torch.full((sz, sz), fill_value<span class="op">=</span><span class="bu">float</span>(<span class="st">'-inf'</span>), device<span class="op">=</span>permutation.device)</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(sz):</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>        mask[permutation[i], permutation[:i]] <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mask</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> handle_eos_masking(condition: Tensor, mask: Tensor, n_head: <span class="bu">int</span>):</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>    b, t <span class="op">=</span> condition.size()</span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> mask.expand(b, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>    condition <span class="op">=</span> condition.unsqueeze(<span class="op">-</span><span class="dv">1</span>).<span class="bu">bool</span>()</span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> torch.where(condition, torch.tensor(<span class="fl">0.0</span>), mask)</span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mask.repeat_interleave(n_head, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TextDecoder(nn.Module):</span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.config <span class="op">=</span> config</span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_head <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> config.n_head</span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tok_embed <span class="op">=</span> nn.Embedding(config.vocab_size, config.n_embed)</span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pos_embed <span class="op">=</span> nn.Embedding(config.block_size, config.n_embed)</span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ln_pos <span class="op">=</span> RMSNorm(config.n_embed)</span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(config.dropout)</span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.blocks <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a>            [ResidualAttentionBlock(config.n_embed, <span class="va">self</span>.n_head, <span class="dv">4</span> <span class="op">*</span> config.n_embed, config.dropout, config.bias, cross_attn<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>)]</span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ln_f <span class="op">=</span> RMSNorm(config.n_embed)</span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lm_head <span class="op">=</span> Linear(config.n_embed, config.vocab_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a>        <span class="co"># weight tying</span></span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tok_embed.weight <span class="op">=</span> <span class="va">self</span>.lm_head.weight</span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a>        mask <span class="op">=</span> nn.Transformer.generate_square_subsequent_mask(config.block_size)</span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">"mask"</span>, mask)</span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.tok_embed.weight.device.<span class="bu">type</span> <span class="op">==</span> <span class="st">'cuda'</span>:</span>
<span id="cb4-69"><a href="#cb4-69" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.tok_embed.weight <span class="op">=</span> <span class="va">self</span>.tok_embed.weight.to(torch.bfloat16)</span>
<span id="cb4-70"><a href="#cb4-70" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.pos_embed.weight <span class="op">=</span> <span class="va">self</span>.pos_embed.weight.to(torch.bfloat16)</span>
<span id="cb4-71"><a href="#cb4-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-72"><a href="#cb4-72" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, xi, targets<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb4-73"><a href="#cb4-73" aria-hidden="true" tabindex="-1"></a>        b, t <span class="op">=</span> x.size()</span>
<span id="cb4-74"><a href="#cb4-74" aria-hidden="true" tabindex="-1"></a>        pos_embed <span class="op">=</span> <span class="va">self</span>.pos_embed(torch.arange(<span class="dv">0</span>, t, device<span class="op">=</span>x.device)) <span class="co"># exclude bos pos</span></span>
<span id="cb4-75"><a href="#cb4-75" aria-hidden="true" tabindex="-1"></a>        pos_embed <span class="op">=</span> pos_embed.expand(b, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>) <span class="co"># (b, t, n_embed)</span></span>
<span id="cb4-76"><a href="#cb4-76" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-77"><a href="#cb4-77" aria-hidden="true" tabindex="-1"></a>        tok_embed <span class="op">=</span> <span class="va">self</span>.tok_embed(x) <span class="op">*</span> np.sqrt(<span class="va">self</span>.config.n_embed) <span class="co"># (b, t, n_embed)</span></span>
<span id="cb4-78"><a href="#cb4-78" aria-hidden="true" tabindex="-1"></a>        bos_embed <span class="op">=</span> tok_embed[:, :<span class="dv">1</span>]</span>
<span id="cb4-79"><a href="#cb4-79" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-80"><a href="#cb4-80" aria-hidden="true" tabindex="-1"></a>        ctx <span class="op">=</span> torch.cat([bos_embed, tok_embed[:, <span class="dv">1</span>:] <span class="op">+</span> pos_embed[:, :<span class="op">-</span><span class="dv">1</span>]], dim<span class="op">=</span><span class="dv">1</span>) <span class="co"># start from bos token</span></span>
<span id="cb4-81"><a href="#cb4-81" aria-hidden="true" tabindex="-1"></a>        ctx <span class="op">=</span> <span class="va">self</span>.dropout(F.rms_norm(ctx, (ctx.size(<span class="op">-</span><span class="dv">1</span>), ))) <span class="co"># (b, t, n_embed)</span></span>
<span id="cb4-82"><a href="#cb4-82" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-83"><a href="#cb4-83" aria-hidden="true" tabindex="-1"></a>        pos_embed <span class="op">=</span> <span class="va">self</span>.dropout(<span class="va">self</span>.ln_pos(pos_embed))</span>
<span id="cb4-84"><a href="#cb4-84" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> targets <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb4-85"><a href="#cb4-85" aria-hidden="true" tabindex="-1"></a>            key_mask <span class="op">=</span> torch.zeros((b, t), device<span class="op">=</span>x.device)</span>
<span id="cb4-86"><a href="#cb4-86" aria-hidden="true" tabindex="-1"></a>            key_mask[targets <span class="op">&lt;</span> <span class="dv">0</span>] <span class="op">=</span> <span class="bu">float</span>(<span class="st">'-inf'</span>)</span>
<span id="cb4-87"><a href="#cb4-87" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb4-88"><a href="#cb4-88" aria-hidden="true" tabindex="-1"></a>            condition <span class="op">=</span> targets <span class="op">==</span> tokenizer.eos_id</span>
<span id="cb4-89"><a href="#cb4-89" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb4-90"><a href="#cb4-90" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> t<span class="op">-</span><span class="dv">1</span> <span class="op">&gt;</span> <span class="dv">2</span>:</span>
<span id="cb4-91"><a href="#cb4-91" aria-hidden="true" tabindex="-1"></a>                permutations <span class="op">=</span> generate_k_permutations(t<span class="op">-</span><span class="dv">1</span>, <span class="dv">6</span>)</span>
<span id="cb4-92"><a href="#cb4-92" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb4-93"><a href="#cb4-93" aria-hidden="true" tabindex="-1"></a>                permutations <span class="op">=</span> generate_k_permutations(t<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb4-94"><a href="#cb4-94" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb4-95"><a href="#cb4-95" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> perm <span class="kw">in</span> permutations:</span>
<span id="cb4-96"><a href="#cb4-96" aria-hidden="true" tabindex="-1"></a>                attn_mask <span class="op">=</span> torch.zeros((t, t), device<span class="op">=</span>x.device)</span>
<span id="cb4-97"><a href="#cb4-97" aria-hidden="true" tabindex="-1"></a>                attn_mask[:<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>:] <span class="op">=</span> generate_mask(torch.tensor(perm, device<span class="op">=</span>x.device))</span>
<span id="cb4-98"><a href="#cb4-98" aria-hidden="true" tabindex="-1"></a>                attn_mask <span class="op">=</span> handle_eos_masking(condition, attn_mask, <span class="va">self</span>.n_head)</span>
<span id="cb4-99"><a href="#cb4-99" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb4-100"><a href="#cb4-100" aria-hidden="true" tabindex="-1"></a>                logits <span class="op">=</span> <span class="va">self</span>.logits(pos_embed.clone(), ctx, xi, attn_mask<span class="op">=</span>attn_mask, key_mask<span class="op">=</span>key_mask)</span>
<span id="cb4-101"><a href="#cb4-101" aria-hidden="true" tabindex="-1"></a>                loss <span class="op">+=</span> F.cross_entropy(logits.view(<span class="op">-</span><span class="dv">1</span>, logits.size(<span class="op">-</span><span class="dv">1</span>)), targets.view(<span class="op">-</span><span class="dv">1</span>), ignore_index<span class="op">=-</span><span class="dv">100</span>, reduction<span class="op">=</span><span class="st">'mean'</span>)</span>
<span id="cb4-102"><a href="#cb4-102" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> loss <span class="op">/</span> <span class="bu">len</span>(permutations)</span>
<span id="cb4-103"><a href="#cb4-103" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> logits, loss</span>
<span id="cb4-104"><a href="#cb4-104" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb4-105"><a href="#cb4-105" aria-hidden="true" tabindex="-1"></a>            mask <span class="op">=</span> <span class="va">self</span>.mask[:t, :t]</span>
<span id="cb4-106"><a href="#cb4-106" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">self</span>.logits(pos_embed, ctx, xi, mask), _</span>
<span id="cb4-107"><a href="#cb4-107" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-108"><a href="#cb4-108" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> logits(<span class="va">self</span>, query, ctx, xi, attn_mask, key_mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb4-109"><a href="#cb4-109" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> block <span class="kw">in</span> <span class="va">self</span>.blocks:</span>
<span id="cb4-110"><a href="#cb4-110" aria-hidden="true" tabindex="-1"></a>            query <span class="op">=</span> block(query, ctx<span class="op">=</span>ctx, xi<span class="op">=</span>xi, attn_mask<span class="op">=</span>attn_mask, key_mask<span class="op">=</span>key_mask)</span>
<span id="cb4-111"><a href="#cb4-111" aria-hidden="true" tabindex="-1"></a>        query <span class="op">=</span> <span class="va">self</span>.ln_f(query)</span>
<span id="cb4-112"><a href="#cb4-112" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.lm_head(query).<span class="bu">float</span>()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="parseq-model" class="level2">
<h2 class="anchored" data-anchor-id="parseq-model">PARSeq Model</h2>
<p>Now, we can finally write <code>PARSeqModel</code> class where image encoder is a <code>Vision Transformer</code> and text decoder is a single Residual Attention Block. When deploy, we also need to develop <code>decode</code> function that outputs text in a given cropped region.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PARSeqModel(nn.Module):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder <span class="op">=</span> ImageEncoder(config)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder <span class="op">=</span> TextDecoder(config)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, img_tensor: Tensor, inp_tokens: Tensor, tgt_tokens: Tensor<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>        xi <span class="op">=</span> <span class="va">self</span>.encoder(img_tensor)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>        logits, loss <span class="op">=</span> <span class="va">self</span>.decoder(inp_tokens, xi, tgt_tokens)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> logits, loss</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>



</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-bautista2022scene" class="csl-entry" role="listitem">
Bautista, Darwin, and Rowel Atienza. 2022. <span>“Scene Text Recognition with Permuted Autoregressive Sequence Models.”</span> In <em>European Conference on Computer Vision</em>, 178–96. Springer.
</div>
<div id="ref-yang2019generalized" class="csl-entry" role="listitem">
Yang, Z, Z Dai, Y Yang, J Carbonell, R Salakhutdinov, and QV Le. 2019. <span>“Generalized Autoregressive Pretraining for Language Understanding. arXiv.”</span> <em>Preprint Posted Online June</em> 19.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>