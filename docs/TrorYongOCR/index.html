<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="KrorngAI">
<meta name="dcterms.date" content="2026-02-19">
<meta name="description" content="Design a Tiny OCR Model for Khmer text">

<title>TrorYong OCR: Meet Your Tiny OCR Model – KrorngAI</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-fc67718bfdb0cd914aa427a0e7a018c1.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="quarto-light">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#abstract" id="toc-abstract" class="nav-link active" data-scroll-target="#abstract">Abstract</a>
  <ul class="collapse">
  <li><a href="#methodology-use-image-encoding-as-a-prefilll-prompt" id="toc-methodology-use-image-encoding-as-a-prefilll-prompt" class="nav-link" data-scroll-target="#methodology-use-image-encoding-as-a-prefilll-prompt">Methodology: Use Image Encoding as a Prefilll Prompt</a>
  <ul class="collapse">
  <li><a href="#model-configuration" id="toc-model-configuration" class="nav-link" data-scroll-target="#model-configuration">Model Configuration</a></li>
  <li><a href="#compared-to-parseq" id="toc-compared-to-parseq" class="nav-link" data-scroll-target="#compared-to-parseq">Compared to PARSeq</a></li>
  <li><a href="#compared-to-dtrocr" id="toc-compared-to-dtrocr" class="nav-link" data-scroll-target="#compared-to-dtrocr">Compared to DTrOCR</a></li>
  </ul></li>
  <li><a href="#fine-tune-tutorial" id="toc-fine-tune-tutorial" class="nav-link" data-scroll-target="#fine-tune-tutorial">Fine-tune Tutorial</a></li>
  <li><a href="#result" id="toc-result" class="nav-link" data-scroll-target="#result">Result</a></li>
  <li><a href="#deployment" id="toc-deployment" class="nav-link" data-scroll-target="#deployment">Deployment</a></li>
  <li><a href="#implementation" id="toc-implementation" class="nav-link" data-scroll-target="#implementation">Implementation</a></li>
  <li><a href="#training-detail" id="toc-training-detail" class="nav-link" data-scroll-target="#training-detail">Training Detail</a>
  <ul class="collapse">
  <li><a href="#hyperparameters" id="toc-hyperparameters" class="nav-link" data-scroll-target="#hyperparameters">Hyperparameters</a></li>
  <li><a href="#weight-initialization" id="toc-weight-initialization" class="nav-link" data-scroll-target="#weight-initialization">Weight Initialization</a></li>
  <li><a href="#datasets" id="toc-datasets" class="nav-link" data-scroll-target="#datasets">Datasets</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">TrorYong OCR: Meet Your Tiny OCR Model</h1>
  <div class="quarto-categories">
    <div class="quarto-category">Science</div>
    <div class="quarto-category">Technology</div>
  </div>
  </div>

<div>
  <div class="description">
    Design a Tiny OCR Model for Khmer text
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>KrorngAI </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">February 19, 2026</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="abstract" class="level1">
<h1>Abstract</h1>
<p>Inspired by PARSeq <span class="citation" data-cites="bautista2022scene">(<a href="#ref-bautista2022scene" role="doc-biblioref">Bautista and Atienza 2022</a>)</span> and DTrOCR <span class="citation" data-cites="fujitake2024dtrocr">(<a href="#ref-fujitake2024dtrocr" role="doc-biblioref">Fujitake 2024</a>)</span>, I design a tiny OCR model called TrorYongOCR for Scene Text Recognition task. A pretrained version is deployed on Huggingface Space <a href="https://huggingface.co/spaces/KrorngAI/KhmerOCRWithTrorYongOCR">here</a>.</p>
<section id="methodology-use-image-encoding-as-a-prefilll-prompt" class="level2">
<h2 class="anchored" data-anchor-id="methodology-use-image-encoding-as-a-prefilll-prompt">Methodology: Use Image Encoding as a Prefilll Prompt</h2>
<p>Inspired by PARSeq and DTrOCR, I design TrorYongOCR as the following: given <span class="math inline">\(L\)</span> transformer blocks</p>
<ul>
<li><span class="math inline">\(L-1\)</span> are encoding blocks to encode a given image</li>
<li>the last block is a decoding block without cross-attention mechanism</li>
<li>for the decoding block,
<ul>
<li>the latent state of an image (the output of encoding layers) is concatenated with the input character embedding (token embedding including bos token plus position embedding) to create context vector, <em>i.e.</em> key and value vectors (think of it like a prefill prompt)</li>
<li>and the input character embedding (token embedding plus position embedding) is used as query vector.</li>
</ul></li>
</ul>
<p>New technologies in Attention mechanism such as Rotary Positional Embedding (RoPE), Sigmoid Linear Unit (SiLU), and Gated Linear Unit (GLU) in MLP of Transformer block are implemented in TrorYongOCR.</p>
<p>TrorYongOCR architecture overview can be found in <a href="#fig-architect" class="quarto-xref">Figure&nbsp;1</a>.</p>
<div id="fig-architect" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-architect-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://raw.githubusercontent.com/Kimang18/KrorngAI/refs/heads/main/tror-yong-ocr/TrorYongOCR.drawio.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-architect-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: TrorYongOCR architecture overview. The input image is transformed into patch embedding. Image embedding is obtained by additioning patch embedding and position embedding. The image embedding is passed through <span class="math inline">\(L-1\)</span> encoder blocks to generate image encoding (latent state). The image encoding is concatenated with character embedding (i.e.&nbsp;token embedding plus position embedding) before undergoing causal self-attention mechanism in the single decoder block to generate visio-lingual encoding (the ultimate latent state). Finally, the linear layer projects the visio-lingual encoding into logit over token space.
</figcaption>
</figure>
</div>
<section id="model-configuration" class="level3">
<h3 class="anchored" data-anchor-id="model-configuration">Model Configuration</h3>
<p>The choice of model configuration can be found below where the image input dimension is <span class="math inline">\((H, W) = (32, 128)\)</span> where <span class="math inline">\(H\)</span> and <span class="math inline">\(W\)</span> are height and width of image respectively, patch size is <span class="math inline">\((4, 8)\)</span>, block size or maximum number of input tokens is <span class="math inline">\(192\)</span>. Transformer configuration is the following: there are <span class="math inline">\(4\)</span> blocks, each has embedding dimension <span class="math inline">\(d_{model}=384\)</span> and <span class="math inline">\(h=6\)</span> heads. In particular, encoding blocks (block <span class="math inline">\(1\)</span> to <span class="math inline">\(3\)</span>) have feedforward dimension <span class="math inline">\(d_{feedforward}=2*d_{model}=768\)</span> and the decoding block has <span class="math inline">\(d_{feedforward}=4*d_{model}=1546\)</span> (see <a href="#tbl-transformer" class="quarto-xref">Table&nbsp;1</a>).</p>
<div id="tbl-transformer" class="striped hover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-transformer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: Configuration of Transformer Blocks of TrorYongOCR
</figcaption>
<div aria-describedby="tbl-transformer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover caption-top table">
<thead>
<tr class="header">
<th>Layer</th>
<th><span class="math inline">\(d_{model}\)</span></th>
<th><span class="math inline">\(h\)</span></th>
<th><span class="math inline">\(d_{feedforward}\)</span></th>
<th style="text-align: right;">Role</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>384</td>
<td>6</td>
<td>768</td>
<td style="text-align: right;">Encoder</td>
</tr>
<tr class="even">
<td>2</td>
<td>384</td>
<td>6</td>
<td>768</td>
<td style="text-align: right;">Encoder</td>
</tr>
<tr class="odd">
<td>3</td>
<td>384</td>
<td>6</td>
<td>768</td>
<td style="text-align: right;">Encoder</td>
</tr>
<tr class="even">
<td>4</td>
<td>384</td>
<td>6</td>
<td>1546</td>
<td style="text-align: right;">Decoder</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>The python code for the configuration is given below.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> TrorYongConfig(</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    img_size<span class="op">=</span>(<span class="dv">32</span>, <span class="dv">128</span>),</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    patch_size<span class="op">=</span>(<span class="dv">4</span>, <span class="dv">8</span>),</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    n_channel<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    vocab_size<span class="op">=</span><span class="bu">len</span>(tokenizer),</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    block_size<span class="op">=</span><span class="dv">192</span>,</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    n_layer<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    n_head<span class="op">=</span><span class="dv">6</span>,</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    n_embed<span class="op">=</span><span class="dv">384</span>,</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    dropout<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    bias<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="compared-to-parseq" class="level3">
<h3 class="anchored" data-anchor-id="compared-to-parseq">Compared to PARSeq</h3>
<p>For PARSeq model which is an encoder-decoder architecture, text decoder uses position embedding as query vector, character embedding (token embedding plus position embedding) as context vector, and the latent state from image encoder as memory for the cross-attention mechanism (see Figure 3 of <span class="citation" data-cites="bautista2022scene">(<a href="#ref-bautista2022scene" role="doc-biblioref">Bautista and Atienza 2022</a>)</span>).</p>
</section>
<section id="compared-to-dtrocr" class="level3">
<h3 class="anchored" data-anchor-id="compared-to-dtrocr">Compared to DTrOCR</h3>
<p>For DTrOCR which is a decoder-only architecture, the image embedding (patch embedding plus position embedding) is concatenated with input character embedding (a [SEP] token is added at the beginning of input character embedding to indicate sequence separation. [SEP] token is equivalent to <code>bos</code> token in TrorYongOCR), and causal self-attention mechanism is applied to the concatenation from layer to layer to generate text autoregressively (see Figure 2 of <span class="citation" data-cites="fujitake2024dtrocr">(<a href="#ref-fujitake2024dtrocr" role="doc-biblioref">Fujitake 2024</a>)</span>).</p>
</section>
</section>
<section id="fine-tune-tutorial" class="level2">
<h2 class="anchored" data-anchor-id="fine-tune-tutorial">Fine-tune Tutorial</h2>
<p>You can find my video tutorial on fine-tuning TrorYongOCR in the video below.</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/3W8P0mByFBY?si=TbGdt_hlDtEb_LNs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="">
</iframe>
</section>
<section id="result" class="level2">
<h2 class="anchored" data-anchor-id="result">Result</h2>
<p>I used different screenshots to test the trained model. For characters with fonts present in the training dataset, the trained model is able to recognize fairly well. However, for any characters with fonts absent from the training dataset, the trained model perform poorly. For instance, character in <code>Khmer Muol</code> font can completely break the trained model regardless of image aspect ratio.</p>
</section>
<section id="deployment" class="level2">
<h2 class="anchored" data-anchor-id="deployment">Deployment</h2>
<p>I deploy the pretrained model (<code>best-model-90epoch.pt</code>) on KrorngAI space <a href="https://huggingface.co/spaces/KrorngAI/KhmerOCRWithTrorYongOCR">here</a>.</p>
</section>
<section id="implementation" class="level2">
<h2 class="anchored" data-anchor-id="implementation">Implementation</h2>
<p>TrorYongOCR is implemented as a <code>PyPI</code> package and can be installed via</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install tror-yong-ocr</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>More detail is given <a href="https://pypi.org/project/tror-yong-ocr/">here</a>.</p>
</section>
<section id="training-detail" class="level2">
<h2 class="anchored" data-anchor-id="training-detail">Training Detail</h2>
<p>The public pretrained weight of TrorYongOCR can be found <a href="https://huggingface.co/KrorngAI/TrorYongOCR">here</a>. It is obtained by training on <a href="https://huggingface.co/datasets/seanghay/khmer-hanuman-100k"><code>seanghay/khmer-hanuman-100k</code></a> and <a href="https://huggingface.co/datasets/SoyVitou/KhmerSynthetic1M"><code>SoyVitou/KhmerSynthetic1M</code></a> datasets. Due to no benchmark datasets for Khmer language, the metric I can share here is that my model achieves a training loss and a validation loss of around <code>0.2</code>.</p>
<section id="hyperparameters" class="level3">
<h3 class="anchored" data-anchor-id="hyperparameters">Hyperparameters</h3>
<p>The hyperparameters regarding the learning rate is given in <a href="#tbl-training" class="quarto-xref">Table&nbsp;2</a>. For optimizer, I used ‘Adaptive Momentum with Weight Decay’ whose hyperparameters can be found in <a href="#tbl-adamw" class="quarto-xref">Table&nbsp;3</a>.</p>
<div id="tbl-training" class="striped hover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-training-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2: Hyperparameters for learning rate. I used gradient accumulation method to save GPU memory. The learning rate is scheduled with a warmup phase of 10% of total steps when it increases to <span class="math inline">\(lr_{max}\)</span> and a cosine annealing phase when it decreases to <span class="math inline">\(lr_{min}\)</span>.
</figcaption>
<div aria-describedby="tbl-training-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover caption-top table">
<colgroup>
<col style="width: 29%">
<col style="width: 17%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 17%">
</colgroup>
<thead>
<tr class="header">
<th>Batch size</th>
<th>Grad Accum Step</th>
<th><span class="math inline">\(lr_{max}\)</span></th>
<th><span class="math inline">\(lr_{min}\)</span></th>
<th>Schedule</th>
<th style="text-align: right;">Warmup</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>256</td>
<td>2</td>
<td><span class="math inline">\(7\times 10^{-3}\)</span></td>
<td><span class="math inline">\(3.5\times 10^{-4}\)</span></td>
<td>cosine</td>
<td style="text-align: right;">10%</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<div id="tbl-adamw" class="striped hover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-adamw-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3: Hyperparameters for Adaptive Momentum with Weight Decay. I chose betas <span class="math inline">\((\beta_1=0.9, \beta_2=0.999)\)</span> as they are the standard value. For <span class="math inline">\(weight\ decay=0.001\)</span>, the choice is due to the fact that TrorYongOCR is a tiny model (with <span class="math inline">\(7\)</span> millions parameters) and it is best to apply less regularization on tiny size model. Finally, the value of <span class="math inline">\(\varepsilon=10^{-8}\)</span> is just a default choice in AdamW of PyTorch.
</figcaption>
<div aria-describedby="tbl-adamw-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover caption-top table">
<thead>
<tr class="header">
<th><span class="math inline">\(\beta_1\)</span></th>
<th><span class="math inline">\(\beta_2\)</span></th>
<th>weight decay</th>
<th style="text-align: right;"><span class="math inline">\(\varepsilon\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(0.9\)</span></td>
<td><span class="math inline">\(0.999\)</span></td>
<td><span class="math inline">\(0.001\)</span></td>
<td style="text-align: right;"><span class="math inline">\(10^{-8}\)</span></td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section>
<section id="weight-initialization" class="level3">
<h3 class="anchored" data-anchor-id="weight-initialization">Weight Initialization</h3>
<p>We initialize weights as what SOTA models reguarly do. The code to initialize the weight is given below.</p>
<p>Exceptionally, for position embedding used in the decoding block, I initialized it with <span class="math inline">\(std=1.0\)</span>.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> init_weights(<span class="va">self</span>, module: nn.Module, name: <span class="bu">str</span> <span class="op">=</span> <span class="st">''</span>, exclude: Sequence[<span class="bu">str</span>] <span class="op">=</span> (<span class="st">''</span>)):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Initialize the weights using the typical initialization schemes used in SOTA models."""</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">any</span>(<span class="bu">map</span>(name.startswith, exclude)):</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(module, nn.Linear):</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>        nn.init.trunc_normal_(module.weight, std<span class="op">=</span><span class="fl">0.02</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> module.bias <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>            nn.init.zeros_(module.bias)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> <span class="bu">isinstance</span>(module, nn.Embedding):</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>        nn.init.trunc_normal_(module.weight, std<span class="op">=</span><span class="fl">0.02</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> module.padding_idx <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>            module.weight.data[module.padding_idx].zero_()</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> <span class="bu">isinstance</span>(module, nn.Conv2d):</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>        nn.init.kaiming_normal_(module.weight)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> module.bias <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>            nn.init.zeros_(module.bias)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> <span class="bu">isinstance</span>(module, (nn.LayerNorm, nn.BatchNorm2d, nn.GroupNorm)):</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>        nn.init.ones_(module.weight)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>        nn.init.zeros_(module.bias)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="datasets" class="level3">
<h3 class="anchored" data-anchor-id="datasets">Datasets</h3>
<p>Both datasets have an aspect ratio, <span class="math inline">\(\frac{W}{H}\)</span>, varies from <span class="math inline">\(1\)</span> to <span class="math inline">\(15\)</span>. Since the ratio of my input image dimension is <span class="math inline">\(\frac{128}{32}=4\)</span>, the images with extreme ratio will be transformed radically. Consequently, the characters inside the images are transformed too excessively which itself impacts the features of characters inside the image. This makes it hard for model to capture meaningful features in the transformed images.</p>
<section id="khmer-hanuman-100k" class="level4">
<h4 class="anchored" data-anchor-id="khmer-hanuman-100k">khmer-hanuman-100k</h4>
<p>This dataset by <code>YatSeanghay</code> contains images with a variety of background colors and character colors. I split this dataset by 9:1 where 90% is used for training set and 10% is used for validation set. I trained TrorYongOCR on this dataset over 80 epochs. This allows me to get a relatively good model.</p>
</section>
<section id="khmersynthetic1m" class="level4">
<h4 class="anchored" data-anchor-id="khmersynthetic1m">KhmerSynthetic1M</h4>
<p><code>KhmerSynthetic1M</code> is a dataset by <code>SoyVitou</code>. This dataset contains images in gray monochromatic color palette (black, white, gray, etc.,). The distribution of the number of tokens, <em>i.e.</em> frequency of each number of tokens, is fairly uniform. In particular, the maximum number of tokens is around <span class="math inline">\(120\)</span>. This implies that there are images with aspect ratio largely higher than <span class="math inline">\(4\)</span>. So, I filtered out any images with a ratio larger than <span class="math inline">\(5\)</span> and used the filtered dataset to train TrorYongOCR. Similarly, <span class="math inline">\(90\%\)</span> is used for training and 10% for validation. The training carried over <span class="math inline">\(20\)</span> epochs, and the model achieves a <code>cer</code> of <code>11.5%</code> on the validation</p>
</section>
<section id="combined-dataset" class="level4">
<h4 class="anchored" data-anchor-id="combined-dataset">Combined dataset</h4>
<p>Finally, I combined <code>hanuman-100k</code> and <code>KhmerSynthetic1M</code> and trained the model for <span class="math inline">\(1\)</span> more epoch. (Note that I filter out any images with an aspect ratio larger than 5). All of my the checkpoints can be found <a href="https://huggingface.co/KrorngAI/TrorYongOCR">here</a>.</p>



</section>
</section>
</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-bautista2022scene" class="csl-entry" role="listitem">
Bautista, Darwin, and Rowel Atienza. 2022. <span>“Scene Text Recognition with Permuted Autoregressive Sequence Models.”</span> In <em>European Conference on Computer Vision</em>, 178–96. Springer.
</div>
<div id="ref-fujitake2024dtrocr" class="csl-entry" role="listitem">
Fujitake, Masato. 2024. <span>“Dtrocr: Decoder-Only Transformer for Optical Character Recognition.”</span> In <em>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</em>, 8025–35.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>