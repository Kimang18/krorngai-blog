[
  {
    "objectID": "PARSeqImplement/index.html",
    "href": "PARSeqImplement/index.html",
    "title": "Scence Text Recognition with Permuted Autoregressive Sequence Models",
    "section": "",
    "text": "I want to re-implement PARSeq (Bautista and Atienza (2022)) (https://github.com/baudm/parseq) for Khmer text. Using Kimang18/khmer-hanuman-small-images, I got cer of \\(70\\%\\) which is not sufficiently good compared to the performance of PARSeq presented in their paper."
  },
  {
    "objectID": "PARSeqImplement/index.html#objective-function",
    "href": "PARSeqImplement/index.html#objective-function",
    "title": "Scence Text Recognition with Permuted Autoregressive Sequence Models",
    "section": "Objective Function",
    "text": "Objective Function\nLet \\({\\mathbf{e}}\\in{\\mathbb{R}}^{T\\times d_m}\\) be the token embedding of \\({\\mathbf{y}}\\in{\\mathbb{R}}^T\\), \\({\\mathbf{p}}\\in{\\mathbb{R}}^{T\\times d_m}\\) be the position embedding and \\({\\mathbf{c}}:={\\mathbf{e}}+{\\mathbf{p}}\\) be the context embedding. \\(d_m\\) is the embedding dimension. Let \\({\\mathbf{M}}_k\\) be the attention mask that reflect the permutation \\({\\mathbf{z}}_k\\) (e.g. if \\(\\forall t\\le T, z_{k,t}=t\\), then the corresponding mask is just the causal mask for left-to-right direction). The model \\(f({\\mathbf{x}}, {\\mathbf{p}}, {\\mathbf{c}}, {\\mathbf{M}})\\) outputs logits in space \\({\\mathbb{R}}^{(T+1)\\times(S+1)}\\) where \\(S\\) is the size of character set (charset) used for training with additional character pertaining to \\([E]\\) token (which marks the end of sequence). Applying softmax on the logits, we get the probability distribution over charset.\nThe loss for a given \\({\\mathbf{M}}_k\\) is just a cross-entropy loss \\({\\mathcal{L}}_{ce}\\left({\\mathbf{y}}, f({\\mathbf{x}}, {\\mathbf{p}}, {\\mathbf{c}}, {\\mathbf{M}}_k)\\right)\\) and the final loss is given by \\[\n{\\mathcal{L}}=\\frac{1}{K}\\sum_{k=1}^K{\\mathcal{L}}_{ce}\\left({\\mathbf{y}}, f({\\mathbf{x}}, {\\mathbf{p}}, {\\mathbf{c}}, {\\mathbf{M}}_k)\\right)\n\\tag{3}\\]\nSo, the main implementation is to generate permutation \\({\\mathbf{z}}_k\\) and the corresponding attention mask \\({\\mathbf{M}}_k\\) for \\(1\\le k\\le K\\).\n\n\n\n\n\n\nFigure 1: PARSeq architecture and training overview. LayerNorm and Dropout layers are ommitted due to space constraints, \\([B]\\), \\([E]\\), and \\([P]\\) stand for beginning-of-sequence (BOS), end-of-sequence (EOS), and padding tokens, respectively. \\(T=25\\) results in 26 distinct position tokens. The position tokens both serve as query vectors and position embeddings for the input context. For \\([B]\\), no position embedding is added. Attention masks are generated from the given permutations and are used only for the context-position attention. \\({\\mathcal{L}}_{ce}\\) pertains to the cross-entropy loss."
  },
  {
    "objectID": "PARSeqImplement/index.html#tricks-in-implementation",
    "href": "PARSeqImplement/index.html#tricks-in-implementation",
    "title": "Scence Text Recognition with Permuted Autoregressive Sequence Models",
    "section": "Tricks in Implementation",
    "text": "Tricks in Implementation\nThere are two tricks to make the PARSeq’s implementation more feasible:\n\nManipulate Attention Mask\nUse empirical version of Equation 2\n\n\nManipulate Attention Mask\nIn PARSeq, the position tokens encode the target position to be predicted, each one having a direct correspondence to a specific position in the output. The model uses position tokens as the query for the attention mechanism, word combined with position tokens as context embedding (namely, key and value), and mask attention to reflect the ordering specified by permutation. Using position tokens as the query allows the model to learn meaningful pattern from PLM(Bautista and Atienza (2022)). This is different from standard AR model where context embedding as the query (aka self-attention mechanism).\n\n\n\nTable 1: Illustration of AR attention masks for each permutation. The table header (with the \\([B]\\) token) pertains to the input context, while the header column (with the \\([E]\\) token at position \\(p_4\\)) corresponds to the output tokens. \\(0\\) means that the output token has conditional dependency on the corresponding input token. \\(-\\inf\\) means that no information flows from input to output. Note that regardless of permutations, the position \\(p_4\\) corresponding to \\([E]\\) token has information from all input tokens and information of \\([B]\\) token flows to all position tokens.\n\n\n\n\n\n\n\n(a) \\([1,2,3]\\)\n\n\n\n\n\n\n\\([B]\\)\n\\(c_1\\)\n\\(c_2\\)\n\\(c_3\\)\n\n\n\n\n\\(p_1\\)\n0\n\\(-\\inf\\)\n\\(-\\inf\\)\n\\(-\\inf\\)\n\n\n\\(p_2\\)\n0\n0\n\\(-\\inf\\)\n\\(-\\inf\\)\n\n\n\\(p_3\\)\n0\n0\n0\n\\(-\\inf\\)\n\n\n\\(p_4\\)\n0\n0\n0\n0\n\n\n\n\n\n\n\n\n\n\n\n(b) \\([3,2,1]\\)\n\n\n\n\n\n\n\\([B]\\)\n\\(c_1\\)\n\\(c_2\\)\n\\(c_3\\)\n\n\n\n\n\\(p_1\\)\n0\n\\(-\\inf\\)\n0\n0\n\n\n\\(p_2\\)\n0\n\\(-\\inf\\)\n\\(-\\inf\\)\n0\n\n\n\\(p_3\\)\n0\n\\(-\\inf\\)\n\\(-\\inf\\)\n\\(-\\inf\\)\n\n\n\\(p_4\\)\n0\n0\n0\n0\n\n\n\n\n\n\n\n\n\n\n\n(c) \\([1,3,2]\\)\n\n\n\n\n\n\n\\([B]\\)\n\\(c_1\\)\n\\(c_2\\)\n\\(c_3\\)\n\n\n\n\n\\(p_1\\)\n0\n\\(-\\inf\\)\n\\(-\\inf\\)\n\\(-\\inf\\)\n\n\n\\(p_2\\)\n0\n0\n\\(-\\inf\\)\n0\n\n\n\\(p_3\\)\n0\n0\n\\(-\\inf\\)\n\\(-\\inf\\)\n\n\n\\(p_4\\)\n0\n0\n0\n0\n\n\n\n\n\n\n\n\n\n\n\n(d) \\([2,3,1]\\)\n\n\n\n\n\n\n\\([B]\\)\n\\(c_1\\)\n\\(c_2\\)\n\\(c_3\\)\n\n\n\n\n\\(p_1\\)\n0\n\\(-\\inf\\)\n0\n0\n\n\n\\(p_2\\)\n0\n\\(-\\inf\\)\n\\(-\\inf\\)\n\\(-\\inf\\)\n\n\n\\(p_3\\)\n0\n\\(-\\inf\\)\n0\n\\(-\\inf\\)\n\n\n\\(p_4\\)\n0\n0\n0\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse Empirical version of Equation 2\nFor \\(T\\)-length text label, there are \\(T!\\) factorizations of likelihood as Equation 2. This is not practical as \\(T\\) tends to be large in practice. Moreover, doing \\(T!\\) factorizations does not always guarantee better performance (Bautista and Atienza (2022)) and could cause training instability. So, the author makes a compromise by choosing only \\(K=6\\) permutations including left-to-right and right-to-left directions."
  },
  {
    "objectID": "PARSeqImplement/index.html#mixed-precision-training",
    "href": "PARSeqImplement/index.html#mixed-precision-training",
    "title": "Scence Text Recognition with Permuted Autoregressive Sequence Models",
    "section": "Mixed-Precision Training",
    "text": "Mixed-Precision Training\nI will use ready-implemented layers in PyTorch. However, those layers are implemented for float32. Since I use bfloat16, I need to re-implement those layers by casting dtype accordingly. If you do not use mixed-precision training, you can ignore the code in this section.\nfrom typing import Optional, Sequence\nimport torch, torch.nn as nn\nfrom torch import Tensor\nfrom dataclasses import dataclass\n\nclass RMSNorm(nn.RMSNorm):\n    def forward(self, x):\n        return super().forward(x.float()).type(x.dtype)\n\n\nclass Linear(nn.Linear):\n    def forward(self, x: Tensor) -&gt; Tensor:\n        return F.linear(x, self.weight.to(x.dtype), None if self.bias is None else self.bias.to(x.dtype))\n\n\nclass MultiheadAttention(nn.MultiheadAttention):\n    def forward(self, query, key, value, key_padding_mask=None, need_weights=False, attn_mask=None):\n        z1, z2 = super().forward(query.float(), key.float(), value.float(), need_weights=need_weights, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n        return z1.type(query.dtype), z2\n\n\n@dataclass\nclass ModelConfig:\n    img_size: Sequence[int]\n    patch_size: Sequence[int]\n    n_channel: int\n    vocab_size: int\n    block_size: int\n    n_layer: int\n    n_head: int\n    n_embed: int\n    dropout: float = 0.0\n    bias: bool = True"
  },
  {
    "objectID": "PARSeqImplement/index.html#image-encoder",
    "href": "PARSeqImplement/index.html#image-encoder",
    "title": "Scence Text Recognition with Permuted Autoregressive Sequence Models",
    "section": "Image Encoder",
    "text": "Image Encoder\nTo facilitate code readibility, I use Vision Transformer implemented in PyTorch for Image Encoder of PARSeq. I follow PARSeq paper by taking image size as \\((128, 32)\\) and patch size as \\((8, 4)\\). So, patch embedding outputs \\(\\frac{128}{8}\\times \\frac{32}{4}=128\\) tokens, each token is a vector of size \\(d_{m}\\).\nfrom timm.models.vision_transformer import PatchEmbed, VisionTransformer\n\nclass ImageEncoder(VisionTransformer):\n    def __init__(self, config):\n        super().__init__(\n            img_size=config.img_size,\n            patch_size=config.patch_size,\n            in_chans=config.n_channel,\n            embed_dim=config.n_embed,\n            depth=config.n_layer,\n            num_heads=config.n_head,\n            mlp_ratio=4,\n            qkv_bias=True,\n            drop_rate=0.0,\n            attn_drop_rate=0.0,\n            drop_path_rate=0.0,\n            embed_layer=PatchEmbed,\n            num_classes=0, # These\n            global_pool='', # disable the\n            class_token=False, # classifier head.\n        )\n\n    def forward(self, x):\n        return self.forward_features(x)"
  },
  {
    "objectID": "PARSeqImplement/index.html#residual-attention-block",
    "href": "PARSeqImplement/index.html#residual-attention-block",
    "title": "Scence Text Recognition with Permuted Autoregressive Sequence Models",
    "section": "Residual Attention Block",
    "text": "Residual Attention Block\nI design this block for self-attention and causal cross-attention.\nimport torch.nn as nn\n\nclass ResidualAttentionBlock(nn.Module):\n    def __init__(self, n_embed, n_head, n_mlp, dropout=0.0, bias=True, activation=None, cross_attn=False):\n        super().__init__()\n        self.n_embed = n_embed\n        self.n_head = n_head\n        self.dropout1 = nn.Dropout(dropout)\n        self.mha = MultiheadAttention(n_embed, n_head, dropout=dropout, bias=bias, batch_first=True)\n        self.dropout2 = nn.Dropout(dropout)\n        self.cross_ln = RMSNorm(n_embed) if cross_attn else None\n        self.cross_mha = MultiheadAttention(n_embed, n_head, dropout=dropout, bias=bias, batch_first=True) if cross_attn else None\n        self.ln = RMSNorm(n_embed)\n        self.ffn = nn.Sequential(\n            Linear(n_embed, n_mlp, bias=bias),\n            nn.GELU(approximate='tanh'),\n            nn.Dropout(dropout),\n            Linear(n_mlp, n_embed, bias=bias)\n        )\n    \n    def forward(self, x, ctx, xi, key_mask, attn_mask):\n        \"\"\"\n        x: normalized query tensor (b, t, n_embed)\n        ctx: normalized context tensor (b, t, n_embed)\n        xi: normalized image feature tensor (b, t, n_embed)\n        \"\"\"\n        b, t, _ = x.size()\n        residual = x\n        x, _ = self.mha(query=x, key=ctx, value=ctx, key_padding_mask=key_mask, need_weights=False, attn_mask=attn_mask)\n        x = residual + self.dropout1(x)\n        if self.cross_mha:\n            residual = x\n            x = self.cross_ln(x)\n            x, _ = self.cross_mha(query=x, key=xi, value=xi, need_weights=False)\n            x = residual + self.dropout2(x)\n        residual = x\n        x = self.ln(x)\n        x = self.ffn(x)\n        x = residual + x\n        return x"
  },
  {
    "objectID": "PARSeqImplement/index.html#visio-lingual-decoder",
    "href": "PARSeqImplement/index.html#visio-lingual-decoder",
    "title": "Scence Text Recognition with Permuted Autoregressive Sequence Models",
    "section": "Visio-lingual Decoder",
    "text": "Visio-lingual Decoder\nFor visio-lingual decoder, the heavy task is to implement attention mask according to the position permutation. As shown in Table 1, the first column of the mask is for bos. Since any positions must communicate with bos token, the first column of any attention masks here is all zero. Moreover, the position corresponding to eos target token must communicate with all context tokens. So, the last row of any attention masks here is all zero. However, it is possible that the last position tokens correspond to pad target token. In such case, handle_eos_masking function makes sure that the positions corresponding to eos target token communicate with all context tokens.\ndef generate_k_permutations(n, k):\n    \"\"\"\n    Generates K unique permutations of the sequence [0, 1, ..., n-1].\n\n    Args:\n        n (int): The upper bound of the range (0 to n-1).\n        k (int): The number of permutations to generate.\n\n    Returns:\n        list: A list containing K lists, each being a unique permutation.\n    \"\"\"\n    # Create the base sequence\n    base_list = list(range(n))\n\n    # Use a set to ensure we get unique permutations if K is large\n    # relative to the total possible permutations (n!)\n    permutations = set([tuple(base_list), tuple(reversed(base_list))])\n\n    # Calculate n factorial to prevent infinite loops if K &gt; n!\n    max_possible = math.factorial(n)\n    if k &gt; max_possible:\n        raise ValueError(f\"Requested {k} permutations, but only {max_possible} exist for n={n}.\")\n\n    while len(permutations) &lt; k:\n        # random.sample creates a new shuffled list without mutating the original\n        perm = random.sample(base_list, len(base_list))\n        permutations.add(tuple(perm))\n        permutations.add(tuple(reversed(perm)))\n\n    return [list(p) for p in permutations]\n\n\ndef generate_mask(permutation: Tensor):\n    sz = permutation.size(0)\n    mask = torch.full((sz, sz), fill_value=float('-inf'), device=permutation.device)\n    for i in range(sz):\n        mask[permutation[i], permutation[:i]] = 0.0\n    return mask\n\n\ndef handle_eos_masking(condition: Tensor, mask: Tensor, n_head: int):\n    b, t = condition.size()\n    mask = mask.expand(b, -1, -1)\n    condition = condition.unsqueeze(-1).bool()\n    mask = torch.where(condition, torch.tensor(0.0), mask)\n    return mask.repeat_interleave(n_head, dim=0)\n\n\nclass TextDecoder(nn.Module):\n    def __init__(self, config) -&gt; None:\n        super().__init__()\n        self.config = config\n        self.n_head = 2 * config.n_head\n        self.tok_embed = nn.Embedding(config.vocab_size, config.n_embed)\n        self.pos_embed = nn.Embedding(config.block_size, config.n_embed)\n        self.ln_pos = RMSNorm(config.n_embed)\n        self.dropout = nn.Dropout(config.dropout)\n        self.blocks = nn.ModuleList(\n            [ResidualAttentionBlock(config.n_embed, self.n_head, 4 * config.n_embed, config.dropout, config.bias, cross_attn=True)\n            for _ in range(1)]\n        )\n        self.ln_f = RMSNorm(config.n_embed)\n        self.lm_head = Linear(config.n_embed, config.vocab_size, bias=False)\n        # weight tying\n        self.tok_embed.weight = self.lm_head.weight\n        mask = nn.Transformer.generate_square_subsequent_mask(config.block_size)\n        self.register_buffer(\"mask\", mask)\n        if self.tok_embed.weight.device.type == 'cuda':\n            self.tok_embed.weight = self.tok_embed.weight.to(torch.bfloat16)\n            self.pos_embed.weight = self.pos_embed.weight.to(torch.bfloat16)\n\n    def forward(self, x, xi, targets=None):\n        b, t = x.size()\n        pos_embed = self.pos_embed(torch.arange(0, t, device=x.device)) # exclude bos pos\n        pos_embed = pos_embed.expand(b, -1, -1) # (b, t, n_embed)\n        \n        tok_embed = self.tok_embed(x) * np.sqrt(self.config.n_embed) # (b, t, n_embed)\n        bos_embed = tok_embed[:, :1]\n        \n        ctx = torch.cat([bos_embed, tok_embed[:, 1:] + pos_embed[:, :-1]], dim=1) # start from bos token\n        ctx = self.dropout(F.rms_norm(ctx, (ctx.size(-1), ))) # (b, t, n_embed)\n        \n        pos_embed = self.dropout(self.ln_pos(pos_embed))\n        if targets is not None:\n            key_mask = torch.zeros((b, t), device=x.device)\n            key_mask[targets &lt; 0] = float('-inf')\n            \n            condition = targets == tokenizer.eos_id\n            \n            if t-1 &gt; 2:\n                permutations = generate_k_permutations(t-1, 6)\n            else:\n                permutations = generate_k_permutations(t-1, 2)\n            loss = 0.0\n            for perm in permutations:\n                attn_mask = torch.zeros((t, t), device=x.device)\n                attn_mask[:-1, 1:] = generate_mask(torch.tensor(perm, device=x.device))\n                attn_mask = handle_eos_masking(condition, attn_mask, self.n_head)\n                \n                logits = self.logits(pos_embed.clone(), ctx, xi, attn_mask=attn_mask, key_mask=key_mask)\n                loss += F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-100, reduction='mean')\n            loss = loss / len(permutations)\n            return logits, loss\n        else:\n            mask = self.mask[:t, :t]\n            return self.logits(pos_embed, ctx, xi, mask), _\n    \n    def logits(self, query, ctx, xi, attn_mask, key_mask=None):\n        for block in self.blocks:\n            query = block(query, ctx=ctx, xi=xi, attn_mask=attn_mask, key_mask=key_mask)\n        query = self.ln_f(query)\n        return self.lm_head(query).float()"
  },
  {
    "objectID": "PARSeqImplement/index.html#parseq-model",
    "href": "PARSeqImplement/index.html#parseq-model",
    "title": "Scence Text Recognition with Permuted Autoregressive Sequence Models",
    "section": "PARSeq Model",
    "text": "PARSeq Model\nNow, we can finally write PARSeqModel class where image encoder is a Vision Transformer and text decoder is a single Residual Attention Block. When deploy, we also need to develop decode function that outputs text in a given cropped region.\nclass PARSeqModel(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.encoder = ImageEncoder(config)\n        self.encoder = TextDecoder(config)\n\n    def forward(self, img_tensor: Tensor, inp_tokens: Tensor, tgt_tokens: Tensor=None):\n        xi = self.encoder(img_tensor)\n        logits, loss = self.decoder(inp_tokens, xi, tgt_tokens)\n        return logits, loss"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ក្រង AI",
    "section": "",
    "text": "While this work comes truly from the heart, each video represents a significant investment of time – from deep-dive research and code preparation to the final narrative and editing process. I am incredibly passionate about sharing this knowledge, but maintaining this level of quality is a major undertaking. If you find these videos helpful and are in a position to do so, please consider supporting my work with a donation. You can click here to donate or scan the QR code below. Your generosity acts as a huge encouragement and helps ensure that I can continue creating in-depth, valuable content for you.\n\n\n\n\n\nUsing Cambodian bank account, you can donate by scanning my ABA QR code here. (or click here. Make sure that receiver’s name is ‘Khun Kim Ang’.)"
  },
  {
    "objectID": "IntroMultiarmed/index.html",
    "href": "IntroMultiarmed/index.html",
    "title": "សេចក្តីផ្តើមនៃ Multi-Armed Bandit",
    "section": "",
    "text": "Multi-armed bandit (MAB) គឺជាបញ្ហាគណិតវិទ្យាមួយដែលត្រូវបានសិក្សាយ៉ាងស៊ីជម្រៅ និងប្រើប្រាស់យ៉ាងទូលំទូលាយ។ MAB ត្រូវបានផ្តួចផ្តើមដំបូងក្នុងសហគមន៍អ្នកស្រាវជ្រាវតាំងពីឆ្នាំ១៩៣៣មកម៉្លេះ (Thompson 1933) ហើយនៅតែបន្តត្រូវបានគេសិក្សារហូតមកដល់សព្វថ្ងៃ (Simchi-Levi and Wang 2023)។ MAB គឺជាគម្រូលេខនៃបញ្ហារស់នៅក្នុងជីវិតយ៉ាងច្រើនដូចជា ការសាកឃ្លីនិក (Clinical Trial (Thompson 1933)) ប្រព័ន្ធផ្តល់យោបល់ (Recommendation system (Lattimore and Szepesvári 2020)) និងការដាក់ផ្សព្វផ្សាយពាណិជ្ជកម្ម (Advert Placement (Lattimore and Szepesvári 2020)) ជាដើម។ នេះបញ្ជាក់ថា MAB គឺជាបញ្ហាមួយដ៏សំខាន់ដែលយើងទាំងអស់គ្នាគួរតែយកចិត្តទុកដាក់ស្វែងយល់។\nក្នុងអត្ថបទនេះ ខ្ញុំនិងបរិយាយព័ត៌មានទាក់ទងនឹង MAB ដោយចាប់ផ្តើមពីការពន្យល់ពីនិយមន័យ និងប្រវត្តិសង្ខេបនៃ MAB ក្នុងផ្នែកទី១។ ផ្នែកទី២បកស្រាយអំពីទ្វេគ្រោះនៃការរុករកនិងការចំរាញ់ (Exploration-Exploitation Dilemma)។ ផ្នែកទី៣នឹងក្តោបលើរង្វាស់រង្វាល់គុណភាពនៃប្រមាណវិធី (Algorithm) ដែលដោះស្រាយបញ្ហា MAB ។ ខ្ញុំនឹងសង្ខេបអត្ថបទឡើងវិញក្នុងផ្នែកទី៤។"
  },
  {
    "objectID": "IntroMultiarmed/index.html#footnotes",
    "href": "IntroMultiarmed/index.html#footnotes",
    "title": "សេចក្តីផ្តើមនៃ Multi-Armed Bandit",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nពាក្យ “ល្អ” ត្រង់នេះចង់សំដៅថាល្អជាឯកសណ្ឋាន ដែលមិនថាការកំណត់របស់បញ្ហាបែបណាទេ(for any problem configurations) ក៏ប្រមាណវិធីនោះមាន Regret ស្មើនឹង \\(O(T^\\alpha)\\) ដោយ\\(\\alpha&lt;1\\) (សូមមើលសម្គាល់ខាងលើ)។↩︎"
  },
  {
    "objectID": "TrorYongOCR/index.html",
    "href": "TrorYongOCR/index.html",
    "title": "TrorYong OCR: Meet Your Tiny OCR Model",
    "section": "",
    "text": "Inspired by PARSeq (Bautista and Atienza 2022) and DTrOCR (Fujitake 2024), I design a tiny OCR model called TrorYongOCR for Scene Text Recognition task. A pretrained version is deployed on Huggingface Space here.\n\n\nInspired by PARSeq and DTrOCR, I design TrorYongOCR as the following: given \\(L\\) transformer blocks\n\n\\(L-1\\) are encoding blocks to encode a given image\nthe last block is a decoding block without cross-attention mechanism\nfor the decoding block,\n\nthe latent state of an image (the output of encoding layers) is concatenated with the input character embedding (token embedding including bos token plus position embedding) to create context vector, i.e. key and value vectors (think of it like a prefill prompt)\nand the input character embedding (token embedding plus position embedding) is used as query vector.\n\n\nNew technologies in Attention mechanism such as Rotary Positional Embedding (RoPE), Sigmoid Linear Unit (SiLU), and Gated Linear Unit (GLU) in MLP of Transformer block are implemented in TrorYongOCR.\nTrorYongOCR architecture overview can be found in Figure 1.\n\n\n\n\n\n\nFigure 1: TrorYongOCR architecture overview. The input image is transformed into patch embedding. Image embedding is obtained by additioning patch embedding and position embedding. The image embedding is passed through \\(L-1\\) encoder blocks to generate image encoding (latent state). The image encoding is concatenated with character embedding (i.e. token embedding plus position embedding) before undergoing causal self-attention mechanism in the single decoder block to generate visio-lingual encoding (the ultimate latent state). Finally, the linear layer projects the visio-lingual encoding into logit over token space.\n\n\n\n\n\nThe choice of model configuration can be found below where the image input dimension is \\((H, W) = (32, 128)\\) where \\(H\\) and \\(W\\) are height and width of image respectively, patch size is \\((4, 8)\\), block size or maximum number of input tokens is \\(192\\). Transformer configuration is the following: there are \\(4\\) blocks, each has embedding dimension \\(d_{model}=384\\) and \\(h=6\\) heads. In particular, encoding blocks (block \\(1\\) to \\(3\\)) have feedforward dimension \\(d_{feedforward}=2*d_{model}=768\\) and the decoding block has \\(d_{feedforward}=4*d_{model}=1546\\) (see Table 1).\n\n\n\nTable 1: Configuration of Transformer Blocks of TrorYongOCR\n\n\n\n\n\nLayer\n\\(d_{model}\\)\n\\(h\\)\n\\(d_{feedforward}\\)\nRole\n\n\n\n\n1\n384\n6\n768\nEncoder\n\n\n2\n384\n6\n768\nEncoder\n\n\n3\n384\n6\n768\nEncoder\n\n\n4\n384\n6\n1546\nDecoder\n\n\n\n\n\n\nThe python code for the configuration is given below.\nconfig = TrorYongConfig(\n    img_size=(32, 128),\n    patch_size=(4, 8),\n    n_channel=3,\n    vocab_size=len(tokenizer),\n    block_size=192,\n    n_layer=4,\n    n_head=6,\n    n_embed=384,\n    dropout=0.1,\n    bias=True,\n)\n\n\n\nFor PARSeq model which is an encoder-decoder architecture, text decoder uses position embedding as query vector, character embedding (token embedding plus position embedding) as context vector, and the latent state from image encoder as memory for the cross-attention mechanism (see Figure 3 of (Bautista and Atienza 2022)).\n\n\n\nFor DTrOCR which is a decoder-only architecture, the image embedding (patch embedding plus position embedding) is concatenated with input character embedding (a [SEP] token is added at the beginning of input character embedding to indicate sequence separation. [SEP] token is equivalent to bos token in TrorYongOCR), and causal self-attention mechanism is applied to the concatenation from layer to layer to generate text autoregressively (see Figure 2 of (Fujitake 2024)).\n\n\n\n\nYou can find my video tutorial on fine-tuning TrorYongOCR in the video below.\n\n\n\n\n\nI used different screenshots to test the trained model. For characters with fonts present in the training dataset, the trained model is able to recognize fairly well. However, for any characters with fonts absent from the training dataset, the trained model perform poorly. For instance, character in Khmer Muol font can completely break the trained model regardless of image aspect ratio.\n\n\n\nI deploy the pretrained model (best-model-90epoch.pt) on KrorngAI space here.\n\n\n\nTrorYongOCR is implemented as a PyPI package and can be installed via\npip install tror-yong-ocr\nMore detail is given here.\n\n\n\nThe public pretrained weight of TrorYongOCR can be found here. It is obtained by training on seanghay/khmer-hanuman-100k and SoyVitou/KhmerSynthetic1M datasets. Due to no benchmark datasets for Khmer language, the metric I can share here is that my model achieves a training loss and a validation loss of around 0.2.\n\n\nThe hyperparameters regarding the learning rate is given in Table 2. For optimizer, I used ‘Adaptive Momentum with Weight Decay’ whose hyperparameters can be found in Table 3.\n\n\n\nTable 2: Hyperparameters for learning rate. I used gradient accumulation method to save GPU memory. The learning rate is scheduled with a warmup phase of 10% of total steps when it increases to \\(lr_{max}\\) and a cosine annealing phase when it decreases to \\(lr_{min}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nBatch size\nGrad Accum Step\n\\(lr_{max}\\)\n\\(lr_{min}\\)\nSchedule\nWarmup\n\n\n\n\n256\n2\n\\(7\\times 10^{-3}\\)\n\\(3.5\\times 10^{-4}\\)\ncosine\n10%\n\n\n\n\n\n\n\n\n\nTable 3: Hyperparameters for Adaptive Momentum with Weight Decay. I chose betas \\((\\beta_1=0.9, \\beta_2=0.999)\\) as they are the standard value. For \\(weight\\ decay=0.001\\), the choice is due to the fact that TrorYongOCR is a tiny model (with \\(7\\) millions parameters) and it is best to apply less regularization on tiny size model. Finally, the value of \\(\\varepsilon=10^{-8}\\) is just a default choice in AdamW of PyTorch.\n\n\n\n\n\n\\(\\beta_1\\)\n\\(\\beta_2\\)\nweight decay\n\\(\\varepsilon\\)\n\n\n\n\n\\(0.9\\)\n\\(0.999\\)\n\\(0.001\\)\n\\(10^{-8}\\)\n\n\n\n\n\n\n\n\n\nWe initialize weights as what SOTA models reguarly do. The code to initialize the weight is given below.\nExceptionally, for position embedding used in the decoding block, I initialized it with \\(std=1.0\\).\ndef init_weights(self, module: nn.Module, name: str = '', exclude: Sequence[str] = ('')):\n    \"\"\"Initialize the weights using the typical initialization schemes used in SOTA models.\"\"\"\n    if any(map(name.startswith, exclude)):\n        return\n    if isinstance(module, nn.Linear):\n        nn.init.trunc_normal_(module.weight, std=0.02)\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\n    elif isinstance(module, nn.Embedding):\n        nn.init.trunc_normal_(module.weight, std=0.02)\n        if module.padding_idx is not None:\n            module.weight.data[module.padding_idx].zero_()\n    elif isinstance(module, nn.Conv2d):\n        nn.init.kaiming_normal_(module.weight)\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\n    elif isinstance(module, (nn.LayerNorm, nn.BatchNorm2d, nn.GroupNorm)):\n        nn.init.ones_(module.weight)\n        nn.init.zeros_(module.bias)\n\n\n\nBoth datasets have an aspect ratio, \\(\\frac{W}{H}\\), varies from \\(1\\) to \\(15\\). Since the ratio of my input image dimension is \\(\\frac{128}{32}=4\\), the images with extreme ratio will be transformed radically. Consequently, the characters inside the images are transformed too excessively which itself impacts the features of characters inside the image. This makes it hard for model to capture meaningful features in the transformed images.\n\n\nThis dataset by YatSeanghay contains images with a variety of background colors and character colors. I split this dataset by 9:1 where 90% is used for training set and 10% is used for validation set. I trained TrorYongOCR on this dataset over 80 epochs. This allows me to get a relatively good model.\n\n\n\nKhmerSynthetic1M is a dataset by SoyVitou. This dataset contains images in gray monochromatic color palette (black, white, gray, etc.,). The distribution of the number of tokens, i.e. frequency of each number of tokens, is fairly uniform. In particular, the maximum number of tokens is around \\(120\\). This implies that there are images with aspect ratio largely higher than \\(4\\). So, I filtered out any images with a ratio larger than \\(5\\) and used the filtered dataset to train TrorYongOCR. Similarly, \\(90\\%\\) is used for training and 10% for validation. The training carried over \\(20\\) epochs, and the model achieves a cer of 11.5% on the validation\n\n\n\nFinally, I combined hanuman-100k and KhmerSynthetic1M and trained the model for \\(1\\) more epoch. (Note that I filter out any images with an aspect ratio larger than 5). All of my the checkpoints can be found here."
  },
  {
    "objectID": "TrorYongOCR/index.html#objective-function",
    "href": "TrorYongOCR/index.html#objective-function",
    "title": "TrorYong OCR: Meet Your Tiny OCR Model",
    "section": "Objective Function",
    "text": "Objective Function\nLet \\({\\mathbf{e}}\\in{\\mathbb{R}}^{T\\times d_m}\\) be the token embedding of \\({\\mathbf{y}}\\in{\\mathbb{R}}^T\\), \\({\\mathbf{p}}\\in{\\mathbb{R}}^{T\\times d_m}\\) be the position embedding and \\({\\mathbf{c}}:={\\mathbf{e}}+{\\mathbf{p}}\\) be the context embedding. \\(d_m\\) is the embedding dimension. Let \\({\\mathbf{M}}_k\\) be the attention mask that reflect the permutation \\({\\mathbf{z}}_k\\) (e.g. if \\(\\forall t\\le T, z_{k,t}=t\\), then the corresponding mask is just the causal mask for left-to-right direction). The model \\(f({\\mathbf{x}}, {\\mathbf{p}}, {\\mathbf{c}}, {\\mathbf{M}})\\) outputs logits in space \\({\\mathbb{R}}^{(T+1)\\times(S+1)}\\) where \\(S\\) is the size of character set (charset) used for training with additional character pertaining to \\([E]\\) token (which marks the end of sequence). Applying softmax on the logits, we get the probability distribution over charset.\nThe loss for a given \\({\\mathbf{M}}_k\\) is just a cross-entropy loss \\({\\mathcal{L}}_{ce}\\left({\\mathbf{y}}, f({\\mathbf{x}}, {\\mathbf{p}}, {\\mathbf{c}}, {\\mathbf{M}}_k)\\right)\\) and the final loss is given by \\[\n{\\mathcal{L}}=\\frac{1}{K}\\sum_{k=1}^K{\\mathcal{L}}_{ce}\\left({\\mathbf{y}}, f({\\mathbf{x}}, {\\mathbf{p}}, {\\mathbf{c}}, {\\mathbf{M}}_k)\\right)\n\\tag{3}\\]\nSo, the main implementation is to generate permutation \\({\\mathbf{z}}_k\\) and the corresponding attention mask \\({\\mathbf{M}}_k\\) for \\(1\\le k\\le K\\).\n\n\n\n\n\n\nFigure 2: PARSeq architecture and training overview. LayerNorm and Dropout layers are ommitted due to space constraints, \\([B]\\), \\([E]\\), and \\([P]\\) stand for beginning-of-sequence (BOS), end-of-sequence (EOS), and padding tokens, respectively. \\(T=25\\) results in 26 distinct position tokens. The position tokens both serve as query vectors and position embeddings for the input context. For \\([B]\\), no position embedding is added. Attention masks are generated from the given permutations and are used only for the context-position attention. \\({\\mathcal{L}}_{ce}\\) pertains to the cross-entropy loss."
  },
  {
    "objectID": "TrorYongOCR/index.html#tricks-in-implementation",
    "href": "TrorYongOCR/index.html#tricks-in-implementation",
    "title": "TrorYong OCR: Meet Your Tiny OCR Model",
    "section": "Tricks in Implementation",
    "text": "Tricks in Implementation\nThere are two tricks to make the PARSeq’s implementation more feasible:\n\nManipulate Attention Mask\nUse empirical version of Equation 2\n\n\nManipulate Attention Mask\nIn PARSeq, the position tokens encode the target position to be predicted, each one having a direct correspondence to a specific position in the output. The model uses position tokens as the query for the attention mechanism, word combined with position tokens as context embedding (namely, key and value), and mask attention to reflect the ordering specified by permutation. Using position tokens as the query allows the model to learn meaningful pattern from PLM(Bautista and Atienza (2022)). This is different from standard AR model where context embedding as the query (aka self-attention mechanism).\n\n\n\nTable 1: Illustration of AR attention masks for each permutation. The table header (with the \\([B]\\) token) pertains to the input context, while the header column (with the \\([E]\\) token at position \\(p_4\\)) corresponds to the output tokens. \\(0\\) means that the output token has conditional dependency on the corresponding input token. \\(-\\inf\\) means that no information flows from input to output. Note that regardless of permutations, the position \\(p_4\\) corresponding to \\([E]\\) token has information from all input tokens and information of \\([B]\\) token flows to all position tokens.\n\n\n\n\n\n\n\n(a) \\([1,2,3]\\)\n\n\n\n\n\n\n\\([B]\\)\n\\(c_1\\)\n\\(c_2\\)\n\\(c_3\\)\n\n\n\n\n\\(p_1\\)\n0\n\\(-\\inf\\)\n\\(-\\inf\\)\n\\(-\\inf\\)\n\n\n\\(p_2\\)\n0\n0\n\\(-\\inf\\)\n\\(-\\inf\\)\n\n\n\\(p_3\\)\n0\n0\n0\n\\(-\\inf\\)\n\n\n\\(p_4\\)\n0\n0\n0\n0\n\n\n\n\n\n\n\n\n\n\n\n(b) \\([3,2,1]\\)\n\n\n\n\n\n\n\\([B]\\)\n\\(c_1\\)\n\\(c_2\\)\n\\(c_3\\)\n\n\n\n\n\\(p_1\\)\n0\n\\(-\\inf\\)\n0\n0\n\n\n\\(p_2\\)\n0\n\\(-\\inf\\)\n\\(-\\inf\\)\n0\n\n\n\\(p_3\\)\n0\n\\(-\\inf\\)\n\\(-\\inf\\)\n\\(-\\inf\\)\n\n\n\\(p_4\\)\n0\n0\n0\n0\n\n\n\n\n\n\n\n\n\n\n\n(c) \\([1,3,2]\\)\n\n\n\n\n\n\n\\([B]\\)\n\\(c_1\\)\n\\(c_2\\)\n\\(c_3\\)\n\n\n\n\n\\(p_1\\)\n0\n\\(-\\inf\\)\n\\(-\\inf\\)\n\\(-\\inf\\)\n\n\n\\(p_2\\)\n0\n0\n\\(-\\inf\\)\n0\n\n\n\\(p_3\\)\n0\n0\n\\(-\\inf\\)\n\\(-\\inf\\)\n\n\n\\(p_4\\)\n0\n0\n0\n0\n\n\n\n\n\n\n\n\n\n\n\n(d) \\([2,3,1]\\)\n\n\n\n\n\n\n\\([B]\\)\n\\(c_1\\)\n\\(c_2\\)\n\\(c_3\\)\n\n\n\n\n\\(p_1\\)\n0\n\\(-\\inf\\)\n0\n0\n\n\n\\(p_2\\)\n0\n\\(-\\inf\\)\n\\(-\\inf\\)\n\\(-\\inf\\)\n\n\n\\(p_3\\)\n0\n\\(-\\inf\\)\n0\n\\(-\\inf\\)\n\n\n\\(p_4\\)\n0\n0\n0\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse Empirical version of Equation 2\nFor \\(T\\)-length text label, there are \\(T!\\) factorizations of likelihood as Equation 2. This is not practical as \\(T\\) tends to be large in practice. Moreover, doing \\(T!\\) factorizations does not always guarantee better performance (Bautista and Atienza (2022)) and could cause training instability. So, the author makes a compromise by choosing only \\(K=6\\) permutations including left-to-right and right-to-left directions."
  },
  {
    "objectID": "TrorYongOCR/index.html#mixed-precision-training",
    "href": "TrorYongOCR/index.html#mixed-precision-training",
    "title": "TrorYong OCR: Meet Your Tiny OCR Model",
    "section": "",
    "text": "I will use ready-implemented layers in PyTorch. However, those layers are implemented for float32. Since I use bfloat16, I need to re-implement those layers by casting dtype accordingly. If you do not use mixed-precision training, you can ignore the code in this section.\nfrom typing import Optional, Sequence\nimport torch, torch.nn as nn\nfrom torch import Tensor\nfrom dataclasses import dataclass\n\nclass RMSNorm(nn.RMSNorm):\n    def forward(self, x):\n        return super().forward(x.float()).type(x.dtype)\n\n\nclass Linear(nn.Linear):\n    def forward(self, x: Tensor) -&gt; Tensor:\n        return F.linear(x, self.weight.to(x.dtype), None if self.bias is None else self.bias.to(x.dtype))\n\n\nclass MultiheadAttention(nn.MultiheadAttention):\n    def forward(self, query, key, value, key_padding_mask=None, need_weights=False, attn_mask=None):\n        z1, z2 = super().forward(query.float(), key.float(), value.float(), need_weights=need_weights, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n        return z1.type(query.dtype), z2\n\n\n@dataclass\nclass ModelConfig:\n    img_size: Sequence[int]\n    patch_size: Sequence[int]\n    n_channel: int\n    vocab_size: int\n    block_size: int\n    n_layer: int\n    n_head: int\n    n_embed: int\n    dropout: float = 0.0\n    bias: bool = True"
  },
  {
    "objectID": "TrorYongOCR/index.html#image-encoder",
    "href": "TrorYongOCR/index.html#image-encoder",
    "title": "TrorYong OCR: Meet Your Tiny OCR Model",
    "section": "",
    "text": "To facilitate code readibility, I use Vision Transformer implemented in PyTorch for Image Encoder of PARSeq. I follow PARSeq paper by taking image size as \\((128, 32)\\) and patch size as \\((8, 4)\\). So, patch embedding outputs \\(\\frac{128}{8}\\times \\frac{32}{4}=128\\) tokens, each token is a vector of size \\(d_{m}\\).\nfrom timm.models.vision_transformer import PatchEmbed, VisionTransformer\n\nclass ImageEncoder(VisionTransformer):\n    def __init__(self, config):\n        super().__init__(\n            img_size=config.img_size,\n            patch_size=config.patch_size,\n            in_chans=config.n_channel,\n            embed_dim=config.n_embed,\n            depth=config.n_layer,\n            num_heads=config.n_head,\n            mlp_ratio=4,\n            qkv_bias=True,\n            drop_rate=0.0,\n            attn_drop_rate=0.0,\n            drop_path_rate=0.0,\n            embed_layer=PatchEmbed,\n            num_classes=0, # These\n            global_pool='', # disable the\n            class_token=False, # classifier head.\n        )\n\n    def forward(self, x):\n        return self.forward_features(x)"
  },
  {
    "objectID": "TrorYongOCR/index.html#residual-attention-block",
    "href": "TrorYongOCR/index.html#residual-attention-block",
    "title": "TrorYong OCR: Meet Your Tiny OCR Model",
    "section": "",
    "text": "I design this block for self-attention and causal cross-attention.\nimport torch.nn as nn\n\nclass ResidualAttentionBlock(nn.Module):\n    def __init__(self, n_embed, n_head, n_mlp, dropout=0.0, bias=True, activation=None, cross_attn=False):\n        super().__init__()\n        self.n_embed = n_embed\n        self.n_head = n_head\n        self.dropout1 = nn.Dropout(dropout)\n        self.mha = MultiheadAttention(n_embed, n_head, dropout=dropout, bias=bias, batch_first=True)\n        self.dropout2 = nn.Dropout(dropout)\n        self.cross_ln = RMSNorm(n_embed) if cross_attn else None\n        self.cross_mha = MultiheadAttention(n_embed, n_head, dropout=dropout, bias=bias, batch_first=True) if cross_attn else None\n        self.ln = RMSNorm(n_embed)\n        self.ffn = nn.Sequential(\n            Linear(n_embed, n_mlp, bias=bias),\n            nn.GELU(approximate='tanh'),\n            nn.Dropout(dropout),\n            Linear(n_mlp, n_embed, bias=bias)\n        )\n    \n    def forward(self, x, ctx, xi, key_mask, attn_mask):\n        \"\"\"\n        x: normalized query tensor (b, t, n_embed)\n        ctx: normalized context tensor (b, t, n_embed)\n        xi: normalized image feature tensor (b, t, n_embed)\n        \"\"\"\n        b, t, _ = x.size()\n        residual = x\n        x, _ = self.mha(query=x, key=ctx, value=ctx, key_padding_mask=key_mask, need_weights=False, attn_mask=attn_mask)\n        x = residual + self.dropout1(x)\n        if self.cross_mha:\n            residual = x\n            x = self.cross_ln(x)\n            x, _ = self.cross_mha(query=x, key=xi, value=xi, need_weights=False)\n            x = residual + self.dropout2(x)\n        residual = x\n        x = self.ln(x)\n        x = self.ffn(x)\n        x = residual + x\n        return x"
  },
  {
    "objectID": "TrorYongOCR/index.html#visio-lingual-decoder",
    "href": "TrorYongOCR/index.html#visio-lingual-decoder",
    "title": "TrorYong OCR: Meet Your Tiny OCR Model",
    "section": "",
    "text": "For visio-lingual decoder, the heavy task is to implement attention mask according to the position permutation. As shown in ?@tbl-panel, the first column of the mask is for bos. Since any positions must communicate with bos token, the first column of any attention masks here is all zero. Moreover, the position corresponding to eos target token must communicate with all context tokens. So, the last row of any attention masks here is all zero. However, it is possible that the last position tokens correspond to pad target token. In such case, handle_eos_masking function makes sure that the positions corresponding to eos target token communicate with all context tokens.\ndef generate_k_permutations(n, k):\n    \"\"\"\n    Generates K unique permutations of the sequence [0, 1, ..., n-1].\n\n    Args:\n        n (int): The upper bound of the range (0 to n-1).\n        k (int): The number of permutations to generate.\n\n    Returns:\n        list: A list containing K lists, each being a unique permutation.\n    \"\"\"\n    # Create the base sequence\n    base_list = list(range(n))\n\n    # Use a set to ensure we get unique permutations if K is large\n    # relative to the total possible permutations (n!)\n    permutations = set([tuple(base_list), tuple(reversed(base_list))])\n\n    # Calculate n factorial to prevent infinite loops if K &gt; n!\n    max_possible = math.factorial(n)\n    if k &gt; max_possible:\n        raise ValueError(f\"Requested {k} permutations, but only {max_possible} exist for n={n}.\")\n\n    while len(permutations) &lt; k:\n        # random.sample creates a new shuffled list without mutating the original\n        perm = random.sample(base_list, len(base_list))\n        permutations.add(tuple(perm))\n        permutations.add(tuple(reversed(perm)))\n\n    return [list(p) for p in permutations]\n\n\ndef generate_mask(permutation: Tensor):\n    sz = permutation.size(0)\n    mask = torch.full((sz, sz), fill_value=float('-inf'), device=permutation.device)\n    for i in range(sz):\n        mask[permutation[i], permutation[:i]] = 0.0\n    return mask\n\n\ndef handle_eos_masking(condition: Tensor, mask: Tensor, n_head: int):\n    b, t = condition.size()\n    mask = mask.expand(b, -1, -1)\n    condition = condition.unsqueeze(-1).bool()\n    mask = torch.where(condition, torch.tensor(0.0), mask)\n    return mask.repeat_interleave(n_head, dim=0)\n\n\nclass TextDecoder(nn.Module):\n    def __init__(self, config) -&gt; None:\n        super().__init__()\n        self.config = config\n        self.n_head = 2 * config.n_head\n        self.tok_embed = nn.Embedding(config.vocab_size, config.n_embed)\n        self.pos_embed = nn.Embedding(config.block_size, config.n_embed)\n        self.ln_pos = RMSNorm(config.n_embed)\n        self.dropout = nn.Dropout(config.dropout)\n        self.blocks = nn.ModuleList(\n            [ResidualAttentionBlock(config.n_embed, self.n_head, 4 * config.n_embed, config.dropout, config.bias, cross_attn=True)\n            for _ in range(1)]\n        )\n        self.ln_f = RMSNorm(config.n_embed)\n        self.lm_head = Linear(config.n_embed, config.vocab_size, bias=False)\n        # weight tying\n        self.tok_embed.weight = self.lm_head.weight\n        mask = nn.Transformer.generate_square_subsequent_mask(config.block_size)\n        self.register_buffer(\"mask\", mask)\n        if self.tok_embed.weight.device.type == 'cuda':\n            self.tok_embed.weight = self.tok_embed.weight.to(torch.bfloat16)\n            self.pos_embed.weight = self.pos_embed.weight.to(torch.bfloat16)\n\n    def forward(self, x, xi, targets=None):\n        b, t = x.size()\n        pos_embed = self.pos_embed(torch.arange(0, t, device=x.device)) # exclude bos pos\n        pos_embed = pos_embed.expand(b, -1, -1) # (b, t, n_embed)\n        \n        tok_embed = self.tok_embed(x) * np.sqrt(self.config.n_embed) # (b, t, n_embed)\n        bos_embed = tok_embed[:, :1]\n        \n        ctx = torch.cat([bos_embed, tok_embed[:, 1:] + pos_embed[:, :-1]], dim=1) # start from bos token\n        ctx = self.dropout(F.rms_norm(ctx, (ctx.size(-1), ))) # (b, t, n_embed)\n        \n        pos_embed = self.dropout(self.ln_pos(pos_embed))\n        if targets is not None:\n            key_mask = torch.zeros((b, t), device=x.device)\n            key_mask[targets &lt; 0] = float('-inf')\n            \n            condition = targets == tokenizer.eos_id\n            \n            if t-1 &gt; 2:\n                permutations = generate_k_permutations(t-1, 6)\n            else:\n                permutations = generate_k_permutations(t-1, 2)\n            loss = 0.0\n            for perm in permutations:\n                attn_mask = torch.zeros((t, t), device=x.device)\n                attn_mask[:-1, 1:] = generate_mask(torch.tensor(perm, device=x.device))\n                attn_mask = handle_eos_masking(condition, attn_mask, self.n_head)\n                \n                logits = self.logits(pos_embed.clone(), ctx, xi, attn_mask=attn_mask, key_mask=key_mask)\n                loss += F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-100, reduction='mean')\n            loss = loss / len(permutations)\n            return logits, loss\n        else:\n            mask = self.mask[:t, :t]\n            return self.logits(pos_embed, ctx, xi, mask), _\n    \n    def logits(self, query, ctx, xi, attn_mask, key_mask=None):\n        for block in self.blocks:\n            query = block(query, ctx=ctx, xi=xi, attn_mask=attn_mask, key_mask=key_mask)\n        query = self.ln_f(query)\n        return self.lm_head(query).float()"
  },
  {
    "objectID": "TrorYongOCR/index.html#parseq-model",
    "href": "TrorYongOCR/index.html#parseq-model",
    "title": "TrorYong OCR: Meet Your Tiny OCR Model",
    "section": "",
    "text": "Now, we can finally write PARSeqModel class where image encoder is a Vision Transformer and text decoder is a single Residual Attention Block. When deploy, we also need to develop decode function that outputs text in a given cropped region.\nclass PARSeqModel(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.encoder = ImageEncoder(config)\n        self.encoder = TextDecoder(config)\n\n    def forward(self, img_tensor: Tensor, inp_tokens: Tensor, tgt_tokens: Tensor=None):\n        xi = self.encoder(img_tensor)\n        logits, loss = self.decoder(inp_tokens, xi, tgt_tokens)\n        return logits, loss"
  },
  {
    "objectID": "TrorYongOCR/index.html#approach-use-image-encoding-as-a-prefilll-prompt",
    "href": "TrorYongOCR/index.html#approach-use-image-encoding-as-a-prefilll-prompt",
    "title": "TrorYong OCR: Meet Your Tiny OCR Model",
    "section": "Approach: Use Image Encoding as a prefilll prompt",
    "text": "Approach: Use Image Encoding as a prefilll prompt\nInspired by PARSeq and DTrOCR, I design TrorYongOCR as the following: given n_layer transformer layers - n_layer-1 are encoding layers for encoding a given image - the final layer is a decoding layer without cross-attention mechanism - for the decoding layer, - the latent state of an image (the output of encoding layers) is concatenated with the input character embedding (token embedding including bos token plus position embedding) to create context vector, i.e. key and value vectors (think of it like a prompt prefill) - and the input character embedding (token embedding plus position embedding) is used as query vector.\nNew technologies in Attention mechanism such as Rotary Positional Embedding (RoPE), and Sigmoid Linear Unit (SiLU) and Gated Linear Unit (GLU) in MLP of Transformer block are implemented in TrorYongOCR.\n\n\nCompared to PARSeq\nFor PARSeq model which is an encoder-decoder architecture, text decoder uses position embedding as query vector, character embedding (token embedding plus position embedding) as context vector, and the latent state from image encoder as memory for the cross-attention mechanism (see Figure 3 of their paper).\n\n\nCompared to DTrOCR\nFor DTrOCR which is a decoder-only architecture, the image embedding (patch embedding plus position embedding) is concatenated with input character embedding (a [SEP] token is added at the beginning of input character embedding to indicate sequence separation. [SEP] token is equivalent to bos token in TrorYongOCR), and causal self-attention mechanism is applied to the concatenation from layer to layer to generate text autoregressively (see Figure 2 of their paper)."
  },
  {
    "objectID": "TrorYongOCR/index.html#result",
    "href": "TrorYongOCR/index.html#result",
    "title": "TrorYong OCR: Meet Your Tiny OCR Model",
    "section": "",
    "text": "I used different screenshots to test the trained model. For characters with fonts present in the training dataset, the trained model is able to recognize fairly well. However, for any characters with fonts absent from the training dataset, the trained model perform poorly. For instance, character in Khmer Muol font can completely break the trained model regardless of image aspect ratio."
  },
  {
    "objectID": "TrorYongOCR/index.html#deployment",
    "href": "TrorYongOCR/index.html#deployment",
    "title": "TrorYong OCR: Meet Your Tiny OCR Model",
    "section": "",
    "text": "I deploy the pretrained model (best-model-90epoch.pt) on KrorngAI space here."
  },
  {
    "objectID": "TrorYongOCR/index.html#methodology-use-image-encoding-as-a-prefilll-prompt",
    "href": "TrorYongOCR/index.html#methodology-use-image-encoding-as-a-prefilll-prompt",
    "title": "TrorYong OCR: Meet Your Tiny OCR Model",
    "section": "",
    "text": "Inspired by PARSeq and DTrOCR, I design TrorYongOCR as the following: given \\(L\\) transformer blocks\n\n\\(L-1\\) are encoding blocks to encode a given image\nthe last block is a decoding block without cross-attention mechanism\nfor the decoding block,\n\nthe latent state of an image (the output of encoding layers) is concatenated with the input character embedding (token embedding including bos token plus position embedding) to create context vector, i.e. key and value vectors (think of it like a prefill prompt)\nand the input character embedding (token embedding plus position embedding) is used as query vector.\n\n\nNew technologies in Attention mechanism such as Rotary Positional Embedding (RoPE), Sigmoid Linear Unit (SiLU), and Gated Linear Unit (GLU) in MLP of Transformer block are implemented in TrorYongOCR.\nTrorYongOCR architecture overview can be found in Figure 1.\n\n\n\n\n\n\nFigure 1: TrorYongOCR architecture overview. The input image is transformed into patch embedding. Image embedding is obtained by additioning patch embedding and position embedding. The image embedding is passed through \\(L-1\\) encoder blocks to generate image encoding (latent state). The image encoding is concatenated with character embedding (i.e. token embedding plus position embedding) before undergoing causal self-attention mechanism in the single decoder block to generate visio-lingual encoding (the ultimate latent state). Finally, the linear layer projects the visio-lingual encoding into logit over token space.\n\n\n\n\n\nThe choice of model configuration can be found below where the image input dimension is \\((H, W) = (32, 128)\\) where \\(H\\) and \\(W\\) are height and width of image respectively, patch size is \\((4, 8)\\), block size or maximum number of input tokens is \\(192\\). Transformer configuration is the following: there are \\(4\\) blocks, each has embedding dimension \\(d_{model}=384\\) and \\(h=6\\) heads. In particular, encoding blocks (block \\(1\\) to \\(3\\)) have feedforward dimension \\(d_{feedforward}=2*d_{model}=768\\) and the decoding block has \\(d_{feedforward}=4*d_{model}=1546\\) (see Table 1).\n\n\n\nTable 1: Configuration of Transformer Blocks of TrorYongOCR\n\n\n\n\n\nLayer\n\\(d_{model}\\)\n\\(h\\)\n\\(d_{feedforward}\\)\nRole\n\n\n\n\n1\n384\n6\n768\nEncoder\n\n\n2\n384\n6\n768\nEncoder\n\n\n3\n384\n6\n768\nEncoder\n\n\n4\n384\n6\n1546\nDecoder\n\n\n\n\n\n\nThe python code for the configuration is given below.\nconfig = TrorYongConfig(\n    img_size=(32, 128),\n    patch_size=(4, 8),\n    n_channel=3,\n    vocab_size=len(tokenizer),\n    block_size=192,\n    n_layer=4,\n    n_head=6,\n    n_embed=384,\n    dropout=0.1,\n    bias=True,\n)\n\n\n\nFor PARSeq model which is an encoder-decoder architecture, text decoder uses position embedding as query vector, character embedding (token embedding plus position embedding) as context vector, and the latent state from image encoder as memory for the cross-attention mechanism (see Figure 3 of (Bautista and Atienza 2022)).\n\n\n\nFor DTrOCR which is a decoder-only architecture, the image embedding (patch embedding plus position embedding) is concatenated with input character embedding (a [SEP] token is added at the beginning of input character embedding to indicate sequence separation. [SEP] token is equivalent to bos token in TrorYongOCR), and causal self-attention mechanism is applied to the concatenation from layer to layer to generate text autoregressively (see Figure 2 of (Fujitake 2024))."
  },
  {
    "objectID": "TrorYongOCR/index.html#implementation",
    "href": "TrorYongOCR/index.html#implementation",
    "title": "TrorYong OCR: Meet Your Tiny OCR Model",
    "section": "",
    "text": "TrorYongOCR is implemented as a PyPI package and can be installed via\npip install tror-yong-ocr\nMore detail is given here."
  },
  {
    "objectID": "TrorYongOCR/index.html#training-detail",
    "href": "TrorYongOCR/index.html#training-detail",
    "title": "TrorYong OCR: Meet Your Tiny OCR Model",
    "section": "",
    "text": "The public pretrained weight of TrorYongOCR can be found here. It is obtained by training on seanghay/khmer-hanuman-100k and SoyVitou/KhmerSynthetic1M datasets. Due to no benchmark datasets for Khmer language, the metric I can share here is that my model achieves a training loss and a validation loss of around 0.2.\n\n\nThe hyperparameters regarding the learning rate is given in Table 2. For optimizer, I used ‘Adaptive Momentum with Weight Decay’ whose hyperparameters can be found in Table 3.\n\n\n\nTable 2: Hyperparameters for learning rate. I used gradient accumulation method to save GPU memory. The learning rate is scheduled with a warmup phase of 10% of total steps when it increases to \\(lr_{max}\\) and a cosine annealing phase when it decreases to \\(lr_{min}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nBatch size\nGrad Accum Step\n\\(lr_{max}\\)\n\\(lr_{min}\\)\nSchedule\nWarmup\n\n\n\n\n256\n2\n\\(7\\times 10^{-3}\\)\n\\(3.5\\times 10^{-4}\\)\ncosine\n10%\n\n\n\n\n\n\n\n\n\nTable 3: Hyperparameters for Adaptive Momentum with Weight Decay. I chose betas \\((\\beta_1=0.9, \\beta_2=0.999)\\) as they are the standard value. For \\(weight\\ decay=0.001\\), the choice is due to the fact that TrorYongOCR is a tiny model (with \\(7\\) millions parameters) and it is best to apply less regularization on tiny size model. Finally, the value of \\(\\varepsilon=10^{-8}\\) is just a default choice in AdamW of PyTorch.\n\n\n\n\n\n\\(\\beta_1\\)\n\\(\\beta_2\\)\nweight decay\n\\(\\varepsilon\\)\n\n\n\n\n\\(0.9\\)\n\\(0.999\\)\n\\(0.001\\)\n\\(10^{-8}\\)\n\n\n\n\n\n\n\n\n\nWe initialize weights as what SOTA models reguarly do. The code to initialize the weight is given below.\nExceptionally, for position embedding used in the decoding block, I initialized it with \\(std=1.0\\).\ndef init_weights(self, module: nn.Module, name: str = '', exclude: Sequence[str] = ('')):\n    \"\"\"Initialize the weights using the typical initialization schemes used in SOTA models.\"\"\"\n    if any(map(name.startswith, exclude)):\n        return\n    if isinstance(module, nn.Linear):\n        nn.init.trunc_normal_(module.weight, std=0.02)\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\n    elif isinstance(module, nn.Embedding):\n        nn.init.trunc_normal_(module.weight, std=0.02)\n        if module.padding_idx is not None:\n            module.weight.data[module.padding_idx].zero_()\n    elif isinstance(module, nn.Conv2d):\n        nn.init.kaiming_normal_(module.weight)\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\n    elif isinstance(module, (nn.LayerNorm, nn.BatchNorm2d, nn.GroupNorm)):\n        nn.init.ones_(module.weight)\n        nn.init.zeros_(module.bias)\n\n\n\nBoth datasets have an aspect ratio, \\(\\frac{W}{H}\\), varies from \\(1\\) to \\(15\\). Since the ratio of my input image dimension is \\(\\frac{128}{32}=4\\), the images with extreme ratio will be transformed radically. Consequently, the characters inside the images are transformed too excessively which itself impacts the features of characters inside the image. This makes it hard for model to capture meaningful features in the transformed images.\n\n\nThis dataset by YatSeanghay contains images with a variety of background colors and character colors. I split this dataset by 9:1 where 90% is used for training set and 10% is used for validation set. I trained TrorYongOCR on this dataset over 80 epochs. This allows me to get a relatively good model.\n\n\n\nKhmerSynthetic1M is a dataset by SoyVitou. This dataset contains images in gray monochromatic color palette (black, white, gray, etc.,). The distribution of the number of tokens, i.e. frequency of each number of tokens, is fairly uniform. In particular, the maximum number of tokens is around \\(120\\). This implies that there are images with aspect ratio largely higher than \\(4\\). So, I filtered out any images with a ratio larger than \\(5\\) and used the filtered dataset to train TrorYongOCR. Similarly, \\(90\\%\\) is used for training and 10% for validation. The training carried over \\(20\\) epochs, and the model achieves a cer of 11.5% on the validation\n\n\n\nFinally, I combined hanuman-100k and KhmerSynthetic1M and trained the model for \\(1\\) more epoch. (Note that I filter out any images with an aspect ratio larger than 5). All of my the checkpoints can be found here."
  },
  {
    "objectID": "TrorYongOCR/index.html#fine-tune-tutorial",
    "href": "TrorYongOCR/index.html#fine-tune-tutorial",
    "title": "TrorYong OCR: Meet Your Tiny OCR Model",
    "section": "",
    "text": "You can find my video tutorial on fine-tuning TrorYongOCR in the video below."
  },
  {
    "objectID": "index.html#support-my-work",
    "href": "index.html#support-my-work",
    "title": "ក្រង AI",
    "section": "",
    "text": "While this work comes truly from the heart, each video represents a significant investment of time – from deep-dive research and code preparation to the final narrative and editing process. I am incredibly passionate about sharing this knowledge, but maintaining this level of quality is a major undertaking. If you find these videos helpful and are in a position to do so, please consider supporting my work with a donation. You can click here to donate or scan the QR code below. Your generosity acts as a huge encouragement and helps ensure that I can continue creating in-depth, valuable content for you.\n\n\n\n\n\nUsing Cambodian bank account, you can donate by scanning my ABA QR code here. (or click here. Make sure that receiver’s name is ‘Khun Kim Ang’.)"
  },
  {
    "objectID": "index.html#recent-posts",
    "href": "index.html#recent-posts",
    "title": "ក្រង AI",
    "section": "Recent Posts",
    "text": "Recent Posts"
  },
  {
    "objectID": "index.html#pypi-packages",
    "href": "index.html#pypi-packages",
    "title": "ក្រង AI",
    "section": "PyPI Packages",
    "text": "PyPI Packages\n\nTrorYongOCR: Optical Character Recognition Models with my design of model architecture (pip install tror-yong-ocr or more info here)\nNeoWhisper: Re-implementation of Whisper with the most recent attention technologies (pip install neo-whisper or more info here)\nTrorYongGPT: Small Language Models with the most recent attention technologies implemented (pip install tror-yong-lm or more info here)"
  }
]