[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "KrorngAI",
    "section": "",
    "text": "Scence Text Recognition with Permuted Autoregressive Sequence Models\n\n\nRe-implementing PARSeq Model from scratch for Khmer text\n\n\n\n\n\nFeb 1, 2026\n\n\nKrorngAI\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n2"
  },
  {
    "objectID": "PARSeqImplement/index.html",
    "href": "PARSeqImplement/index.html",
    "title": "Scence Text Recognition with Permuted Autoregressive Sequence Models",
    "section": "",
    "text": "I want to re-implement PARSeq (Bautista and Atienza (2022)) (https://github.com/baudm/parseq) for Khmer text. Using Kimang18/khmer-hanuman-small-images, I got cer of \\(70\\%\\) which is not sufficiently good compared to the performance of PARSeq presented in their paper."
  },
  {
    "objectID": "PARSeqImplement/index.html#objective-function",
    "href": "PARSeqImplement/index.html#objective-function",
    "title": "Scence Text Recognition with Permuted Autoregressive Sequence Models",
    "section": "Objective Function",
    "text": "Objective Function\nLet \\({\\mathbf{e}}\\in{\\mathbb{R}}^{T\\times d_m}\\) be the token embedding of \\({\\mathbf{y}}\\in{\\mathbb{R}}^T\\), \\({\\mathbf{p}}\\in{\\mathbb{R}}^{T\\times d_m}\\) be the position embedding and \\({\\mathbf{c}}:={\\mathbf{e}}+{\\mathbf{p}}\\) be the context embedding. \\(d_m\\) is the embedding dimension. Let \\({\\mathbf{M}}_k\\) be the attention mask that reflect the permutation \\({\\mathbf{z}}_k\\) (e.g. if \\(\\forall t\\le T, z_{k,t}=t\\), then the corresponding mask is just the causal mask for left-to-right direction). The model \\(f({\\mathbf{x}}, {\\mathbf{p}}, {\\mathbf{c}}, {\\mathbf{M}})\\) outputs logits in space \\({\\mathbb{R}}^{(T+1)\\times(S+1)}\\) where \\(S\\) is the size of character set (charset) used for training with additional character pertaining to \\([E]\\) token (which marks the end of sequence). Applying softmax on the logits, we get the probability distribution over charset.\nThe loss for a given \\({\\mathbf{M}}_k\\) is just a cross-entropy loss \\({\\mathcal{L}}_{ce}\\left({\\mathbf{y}}, f({\\mathbf{x}}, {\\mathbf{p}}, {\\mathbf{c}}, {\\mathbf{M}}_k)\\right)\\) and the final loss is given by \\[\n{\\mathcal{L}}=\\frac{1}{K}\\sum_{k=1}^K{\\mathcal{L}}_{ce}\\left({\\mathbf{y}}, f({\\mathbf{x}}, {\\mathbf{p}}, {\\mathbf{c}}, {\\mathbf{M}}_k)\\right)\n\\tag{3}\\]\nSo, the main implementation is to generate permutation \\({\\mathbf{z}}_k\\) and the corresponding attention mask \\({\\mathbf{M}}_k\\) for \\(1\\le k\\le K\\).\n\n\n\n\n\n\nFigure 1: PARSeq architecture and training overview. LayerNorm and Dropout layers are ommitted due to space constraints, \\([B]\\), \\([E]\\), and \\([P]\\) stand for beginning-of-sequence (BOS), end-of-sequence (EOS), and padding tokens, respectively. \\(T=25\\) results in 26 distinct position tokens. The position tokens both serve as query vectors and position embeddings for the input context. For \\([B]\\), no position embedding is added. Attention masks are generated from the given permutations and are used only for the context-position attention. \\({\\mathcal{L}}_{ce}\\) pertains to the cross-entropy loss."
  },
  {
    "objectID": "PARSeqImplement/index.html#tricks-in-implementation",
    "href": "PARSeqImplement/index.html#tricks-in-implementation",
    "title": "Scence Text Recognition with Permuted Autoregressive Sequence Models",
    "section": "Tricks in Implementation",
    "text": "Tricks in Implementation\nThere are two tricks to make the PARSeq’s implementation more feasible:\n\nManipulate Attention Mask\nUse empirical version of Equation 2\n\n\nManipulate Attention Mask\nIn PARSeq, the position tokens encode the target position to be predicted, each one having a direct correspondence to a specific position in the output. The model uses position tokens as the query for the attention mechanism, word combined with position tokens as context embedding (namely, key and value), and mask attention to reflect the ordering specified by permutation. Using position tokens as the query allows the model to learn meaningful pattern from PLM(Bautista and Atienza (2022)). This is different from standard AR model where context embedding as the query (aka self-attention mechanism).\n\n\n\nTable 1: Illustration of AR attention masks for each permutation. The table header (with the \\([B]\\) token) pertains to the input context, while the header column (with the \\([E]\\) token at position \\(p_4\\)) corresponds to the output tokens. \\(0\\) means that the output token has conditional dependency on the corresponding input token. \\(-\\inf\\) means that no information flows from input to output. Note that regardless of permutations, the position \\(p_4\\) corresponding to \\([E]\\) token has information from all input tokens and information of \\([B]\\) token flows to all position tokens.\n\n\n\n\n\n\n\n(a) \\([1,2,3]\\)\n\n\n\n\n\n\n\\([B]\\)\n\\(c_1\\)\n\\(c_2\\)\n\\(c_3\\)\n\n\n\n\n\\(p_1\\)\n0\n\\(-\\inf\\)\n\\(-\\inf\\)\n\\(-\\inf\\)\n\n\n\\(p_2\\)\n0\n0\n\\(-\\inf\\)\n\\(-\\inf\\)\n\n\n\\(p_3\\)\n0\n0\n0\n\\(-\\inf\\)\n\n\n\\(p_4\\)\n0\n0\n0\n0\n\n\n\n\n\n\n\n\n\n\n\n(b) \\([3,2,1]\\)\n\n\n\n\n\n\n\\([B]\\)\n\\(c_1\\)\n\\(c_2\\)\n\\(c_3\\)\n\n\n\n\n\\(p_1\\)\n0\n\\(-\\inf\\)\n0\n0\n\n\n\\(p_2\\)\n0\n\\(-\\inf\\)\n\\(-\\inf\\)\n0\n\n\n\\(p_3\\)\n0\n\\(-\\inf\\)\n\\(-\\inf\\)\n\\(-\\inf\\)\n\n\n\\(p_4\\)\n0\n0\n0\n0\n\n\n\n\n\n\n\n\n\n\n\n(c) \\([1,3,2]\\)\n\n\n\n\n\n\n\\([B]\\)\n\\(c_1\\)\n\\(c_2\\)\n\\(c_3\\)\n\n\n\n\n\\(p_1\\)\n0\n\\(-\\inf\\)\n\\(-\\inf\\)\n\\(-\\inf\\)\n\n\n\\(p_2\\)\n0\n0\n\\(-\\inf\\)\n0\n\n\n\\(p_3\\)\n0\n0\n\\(-\\inf\\)\n\\(-\\inf\\)\n\n\n\\(p_4\\)\n0\n0\n0\n0\n\n\n\n\n\n\n\n\n\n\n\n(d) \\([2,3,1]\\)\n\n\n\n\n\n\n\\([B]\\)\n\\(c_1\\)\n\\(c_2\\)\n\\(c_3\\)\n\n\n\n\n\\(p_1\\)\n0\n\\(-\\inf\\)\n0\n0\n\n\n\\(p_2\\)\n0\n\\(-\\inf\\)\n\\(-\\inf\\)\n\\(-\\inf\\)\n\n\n\\(p_3\\)\n0\n\\(-\\inf\\)\n0\n\\(-\\inf\\)\n\n\n\\(p_4\\)\n0\n0\n0\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse Empirical version of Equation 2\nFor \\(T\\)-length text label, there are \\(T!\\) factorizations of likelihood as Equation 2. This is not practical as \\(T\\) tends to be large in practice. Moreover, doing \\(T!\\) factorizations does not always guarantee better performance (Bautista and Atienza (2022)) and could cause training instability. So, the author makes a compromise by choosing only \\(K=6\\) permutations including left-to-right and right-to-left directions."
  },
  {
    "objectID": "PARSeqImplement/index.html#mixed-precision-training",
    "href": "PARSeqImplement/index.html#mixed-precision-training",
    "title": "Scence Text Recognition with Permuted Autoregressive Sequence Models",
    "section": "Mixed-Precision Training",
    "text": "Mixed-Precision Training\nI will use ready-implemented layers in PyTorch. However, those layers are implemented for float32. Since I use bfloat16, I need to re-implement those layers by casting dtype accordingly. If you do not use mixed-precision training, you can ignore the code in this section.\nfrom typing import Optional, Sequence\nimport torch, torch.nn as nn\nfrom torch import Tensor\nfrom dataclasses import dataclass\n\nclass RMSNorm(nn.RMSNorm):\n    def forward(self, x):\n        return super().forward(x.float()).type(x.dtype)\n\n\nclass Linear(nn.Linear):\n    def forward(self, x: Tensor) -&gt; Tensor:\n        return F.linear(x, self.weight.to(x.dtype), None if self.bias is None else self.bias.to(x.dtype))\n\n\nclass MultiheadAttention(nn.MultiheadAttention):\n    def forward(self, query, key, value, key_padding_mask=None, need_weights=False, attn_mask=None):\n        z1, z2 = super().forward(query.float(), key.float(), value.float(), need_weights=need_weights, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n        return z1.type(query.dtype), z2\n\n\n@dataclass\nclass ModelConfig:\n    img_size: Sequence[int]\n    patch_size: Sequence[int]\n    n_channel: int\n    vocab_size: int\n    block_size: int\n    n_layer: int\n    n_head: int\n    n_embed: int\n    dropout: float = 0.0\n    bias: bool = True"
  },
  {
    "objectID": "PARSeqImplement/index.html#image-encoder",
    "href": "PARSeqImplement/index.html#image-encoder",
    "title": "Scence Text Recognition with Permuted Autoregressive Sequence Models",
    "section": "Image Encoder",
    "text": "Image Encoder\nTo facilitate code readibility, I use Vision Transformer implemented in PyTorch for Image Encoder of PARSeq. I follow PARSeq paper by taking image size as \\((128, 32)\\) and patch size as \\((8, 4)\\). So, patch embedding outputs \\(\\frac{128}{8}\\times \\frac{32}{4}=128\\) tokens, each token is a vector of size \\(d_{m}\\).\nfrom timm.models.vision_transformer import PatchEmbed, VisionTransformer\n\nclass ImageEncoder(VisionTransformer):\n    def __init__(self, config):\n        super().__init__(\n            img_size=config.img_size,\n            patch_size=config.patch_size,\n            in_chans=config.n_channel,\n            embed_dim=config.n_embed,\n            depth=config.n_layer,\n            num_heads=config.n_head,\n            mlp_ratio=2,\n            qkv_bias=False,\n            qk_norm=True,\n            proj_bias=config.bias,\n            drop_rate=config.dropout,\n            attn_drop_rate=config.dropout,\n            embed_layer=PatchEmbed,\n            pre_norm=True,\n            final_norm=True,\n            norm_layer=RMSNorm,\n            num_classes=0, # These\n            global_pool='', # disable the\n            class_token=False, # classifier head.\n        )\n\n    def forward(self, x):\n        return self.forward_features(x.float()).type(x.dtype)"
  },
  {
    "objectID": "PARSeqImplement/index.html#residual-attention-block",
    "href": "PARSeqImplement/index.html#residual-attention-block",
    "title": "Scence Text Recognition with Permuted Autoregressive Sequence Models",
    "section": "Residual Attention Block",
    "text": "Residual Attention Block\nI design this block for self-attention and causal cross-attention.\nimport torch.nn as nn\n\nclass ResidualAttentionBlock(nn.Module):\n    def __init__(self, n_embed, n_head, n_mlp, dropout=0.0, bias=True, activation=None, cross_attn=False):\n        super().__init__()\n        self.n_embed = n_embed\n        self.n_head = n_head\n        self.dropout1 = nn.Dropout(dropout)\n        self.mha = MultiheadAttention(n_embed, n_head, dropout=dropout, bias=bias, batch_first=True)\n        self.dropout2 = nn.Dropout(dropout)\n        self.cross_ln = RMSNorm(n_embed) if cross_attn else None\n        self.cross_mha = MultiheadAttention(n_embed, n_head, dropout=dropout, bias=bias, batch_first=True) if cross_attn else None\n        self.ln = RMSNorm(n_embed)\n        self.ffn = nn.Sequential(\n            Linear(n_embed, n_mlp, bias=bias),\n            nn.GELU(approximate='tanh'),\n            nn.Dropout(dropout),\n            Linear(n_mlp, n_embed, bias=bias)\n        )\n    \n    def forward(self, x, ctx, xi, key_mask, attn_mask):\n        \"\"\"\n        x: normalized query tensor (b, t, n_embed)\n        ctx: normalized context tensor (b, t, n_embed)\n        xi: normalized image feature tensor (b, t, n_embed)\n        \"\"\"\n        b, t, _ = x.size()\n        residual = x\n        x, _ = self.mha(query=x, key=ctx, value=ctx, key_padding_mask=key_mask, need_weights=False, attn_mask=attn_mask)\n        x = residual + self.dropout1(x)\n        if self.cross_mha:\n            residual = x\n            x = self.cross_ln(x)\n            x, _ = self.cross_mha(query=x, key=xi, value=xi, need_weights=False)\n            x = residual + self.dropout2(x)\n        residual = x\n        x = self.ln(x)\n        x = self.ffn(x)\n        x = residual + x\n        return x"
  },
  {
    "objectID": "PARSeqImplement/index.html#visio-lingual-decoder",
    "href": "PARSeqImplement/index.html#visio-lingual-decoder",
    "title": "Scence Text Recognition with Permuted Autoregressive Sequence Models",
    "section": "Visio-lingual Decoder",
    "text": "Visio-lingual Decoder\ndef generate_k_permutations(n, k):\n    \"\"\"\n    Generates K unique permutations of the sequence [0, 1, ..., n-1].\n\n    Args:\n        n (int): The upper bound of the range (0 to n-1).\n        k (int): The number of permutations to generate.\n\n    Returns:\n        list: A list containing K lists, each being a unique permutation.\n    \"\"\"\n    # Create the base sequence\n    base_list = list(range(n))\n\n    # Use a set to ensure we get unique permutations if K is large\n    # relative to the total possible permutations (n!)\n    permutations = set([tuple(base_list), tuple(reversed(base_list))])\n\n    # Calculate n factorial to prevent infinite loops if K &gt; n!\n    max_possible = math.factorial(n)\n    if k &gt; max_possible:\n        raise ValueError(f\"Requested {k} permutations, but only {max_possible} exist for n={n}.\")\n\n    while len(permutations) &lt; k:\n        # random.sample creates a new shuffled list without mutating the original\n        perm = random.sample(base_list, len(base_list))\n        permutations.add(tuple(perm))\n        permutations.add(tuple(reversed(perm)))\n\n    return [list(p) for p in permutations]\n\n\ndef generate_mask(permutation: Tensor):\n    sz = permutation.size(0)\n    mask = torch.full((sz, sz), fill_value=float('-inf'), device=permutation.device)\n    for i in range(sz):\n        mask[permutation[i], permutation[:i]] = 0.0\n    return mask\n\n\ndef handle_eos_masking(condition: Tensor, mask: Tensor, n_head: int):\n    b, t = condition.size()\n    mask = mask.expand(b, -1, -1)\n    condition = condition.unsqueeze(-1).bool()\n    mask = torch.where(condition, torch.tensor(0.0), mask)\n    return mask.repeat_interleave(n_head, dim=0)\n\n\nclass TextDecoder(nn.Module):\n    def __init__(self, config) -&gt; None:\n        super().__init__()\n        self.config = config\n        self.n_head = 2 * config.n_head\n        self.tok_embed = nn.Embedding(config.vocab_size, config.n_embed)\n        self.pos_embed = nn.Embedding(config.block_size, config.n_embed)\n        self.ln_pos = RMSNorm(config.n_embed)\n        self.dropout = nn.Dropout(config.dropout)\n        self.blocks = nn.ModuleList(\n            [ResidualAttentionBlock(config.n_embed, self.n_head, 4 * config.n_embed, config.dropout, config.bias, cross_attn=True)\n            for _ in range(1)]\n        )\n        self.ln_f = RMSNorm(config.n_embed)\n        self.lm_head = Linear(config.n_embed, config.vocab_size, bias=False)\n        # weight tying\n        self.tok_embed.weight = self.lm_head.weight\n        mask = nn.Transformer.generate_square_subsequent_mask(config.block_size)\n        self.register_buffer(\"mask\", mask)\n        if self.tok_embed.weight.device.type == 'cuda':\n            self.tok_embed.weight = self.tok_embed.weight.to(torch.bfloat16)\n            self.pos_embed.weight = self.pos_embed.weight.to(torch.bfloat16)\n\n    def forward(self, x, xi, targets=None):\n        b, t = x.size()\n        pos_embed = self.pos_embed(torch.arange(0, t, device=x.device)) # exclude bos pos\n        pos_embed = pos_embed.expand(b, -1, -1) # (b, t, n_embed)\n        \n        tok_embed = self.tok_embed(x) * np.sqrt(self.config.n_embed) # (b, t, n_embed)\n        bos_embed = tok_embed[:, :1]\n        \n        ctx = torch.cat([bos_embed, tok_embed[:, 1:] + pos_embed[:, :-1]], dim=1) # start from bos token\n        ctx = self.dropout(F.rms_norm(ctx, (ctx.size(-1), ))) # (b, t, n_embed)\n        \n        pos_embed = self.dropout(self.ln_pos(pos_embed))\n        if targets is not None:\n            key_mask = torch.zeros((b, t), device=x.device)\n            key_mask[targets &lt; 0] = float('-inf')\n            \n            condition = targets == tokenizer.eos_id\n            \n            if t-1 &gt; 2:\n                permutations = generate_k_permutations(t-1, 6)\n            else:\n                permutations = generate_k_permutations(t-1, 2)\n            loss = 0.0\n            for perm in permutations:\n                attn_mask = torch.zeros((t, t), device=x.device)\n                attn_mask[:-1, 1:] = generate_mask(torch.tensor(perm, device=x.device))\n                attn_mask = handle_eos_masking(condition, attn_mask, self.n_head)\n                \n                logits = self.logits(pos_embed.clone(), ctx, xi, attn_mask=attn_mask, key_mask=key_mask)\n                loss += F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-100, reduction='mean')\n            loss = loss / len(permutations)\n            return logits, loss\n        else:\n            mask = self.mask[:t, :t]\n            return self.logits(pos_embed, ctx, xi, mask), _\n    \n    def logits(self, query, ctx, xi, attn_mask, key_mask=None):\n        for block in self.blocks:\n            query = block(query, ctx=ctx, xi=xi, attn_mask=attn_mask, key_mask=key_mask)\n        query = self.ln_f(query)\n        return self.lm_head(query).float()"
  },
  {
    "objectID": "PARSeqImplement/index.html#parseq-model",
    "href": "PARSeqImplement/index.html#parseq-model",
    "title": "Scence Text Recognition with Permuted Autoregressive Sequence Models",
    "section": "PARSeq Model",
    "text": "PARSeq Model\nclass PARSeqModel(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.encoder = ImageEncoder(config)\n        self.encoder = TextDecoder(config)\n\n    def forward(self, img_tensor: Tensor, inp_tokens: Tensor, tgt_tokens: Tensor=None):\n        xi = self.encoder(img_tensor)\n        logits, loss = self.decoder(inp_tokens, xi, tgt_tokens)\n        return logits, loss"
  }
]